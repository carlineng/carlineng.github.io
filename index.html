<!-- https://github.com/cadars/john-doe/ -->
<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-TYMKFWKMQE"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-TYMKFWKMQE');
  </script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Carlin Eng</title>
  <link rel="shortcut icon" type="image/png" href="./img/favicon.png">
  <meta name="Carlin Eng" content="Homepage of Carlin Eng" />
  <!-- Recommended minimum -->
  <meta property="og:title" content="Carlin Eng" />
  <meta property="og:description" content="Homepage of Carlin Eng" />
  <meta property="og:image" content="img/site-image.webp" />
  <meta name="twitter:card" content="summary_large_image" />
  <link rel="stylesheet" href="style.css" />
  <link rel="stylesheet" href="font-awesome-4.7.0/css/font-awesome.min.css" />
  <link rel="preload" href="./img/site-image.webp" as="image" type="img/jpg" />
</head>

<body>
  <header>
    <h1>
      <a href="#home">Carlin Eng</a>
    </h1>
    <nav>
      <a href="#blog">Blog</a>
      <a href="#pbp">PBP 2019 üö¥‚Äç‚ôÇÔ∏è</a>
      <a href="#about">About</a>
    </nav>
  </header>
  <main>
    <section id="home">
      <!-- HOME -->
      <figure>
        <a href="#img-home">
          <img alt="" src="./img/site-image.webp" width="480" height="360" />
        </a>
      </figure>
      <p> Worn many hats in my career, including Data Engineer, Engineering Manager, Enterprise Sales, and Software
        Engineer; but always in the domain of data and analytics. Previously worked at <a
          href='https://strava.com/'>Strava</a>, <a href='https://www.snowflake.com/'>Snowflake</a>, <a
          href='https://www.geteppo.com'>Eppo</a>, and <a href='https://google.com'>Google</a>. Currently working on the
        <a href='https://malloydata.github.io/documentation/index.html'>Malloy</a> query language at <a
          href='http://meta.com'>Meta</a>. Avid cyclist, and proud member of the Dolphin Club in San Francisco,
        California. </p>
      <div style="position: absolute; bottom: 5%; font-size: .8em; color: #949494"> Made with <span
          id="emoji-carousel">üö¥‚Äç‚ôÇÔ∏è</span> by Carlin Eng <br />
        <div id="social" style="width: fit-content; padding: 5px">
          <a href="https://www.facebook.com/carlin.eng/" class="fa fa-facebook"></a>
          <a href="https://twitter.com/carlineng" class="fa fa-twitter"></a>
          <a href="https://www.linkedin.com/in/carlineng/" class="fa fa-linkedin"></a>
        </div>
      </div>
    </section>
    <section id="about">
      <h1>About</h1>
      <p> This page is based off of the <a href='https://github.com/cadars/john-doe/'>John Doe</a> template, a
        single-file website written only in HTML and CSS, to which I've added small Javascript snippets for toy
        functionality. Source code for this website can be found at <a
          href='https://github.com/carlineng/carlineng.github.io'>Github</a>. <a href="#home">‚Ü©Ô∏è</a>
      </p>
    </section>
    <section id="blog">
      <!-- BLOG -->
      <article>
        <label for="platform-shifts-malloy" class="post-title">Escaping SQL‚Äôs Legacy: Malloy and the era of AI-powered
          Analytics</label>
        <a href="?postid=platform-shifts-malloy#blog" class="selflink">[link]</a>
        <time datetime="2025-04-27">04.27.2025</time>
        <input type="checkbox" id="platform-shifts-malloy" />
        <div class="post-body">
          <p>In the 1980s, a fundamental debate emerged in computer architecture between two competing approaches: CISC
            (Complex Instruction Set Computer) and RISC (Reduced Instruction Set Computer). CISC architectures, backed
            by Intel with leaders such as Pat Gelsinger pushing x86 forward - emphasized powerful, multi-function
            instructions to optimize programmer effort. Meanwhile, RISC architectures, developed by researchers like
            John Hennessy at Stanford, prioritized modularity and speed with simpler instructions. RISC was universally
            considered a technically superior approach, but not strong enough to overcome the immense weight of legacy
            software investments that favored CISC. Enormous amounts of critical software had already been written for
            CISC architectures, and migrating it to RISC would have required costly, large-scale reimplementation - a
            tradeoff few businesses could justify. It was only with the rise of mobile computing in the 2000s - a new
            platform with different constraints - that RISC architectures like ARM finally displaced incumbents,
            following the classic pattern of disruptive innovation. With GenAl, I believe that a similar shift is
            underway in analytics. </p>
          <p> Today, SQL holds a position in the world of analytics very similar to CISC's position in computing decades
            ago: deeply entrenched, widely understood, and supported by a massive ecosystem of tools, systems, and
            talent. This weight of legacy is what has made previous efforts to replace or significantly evolve SQL so
            difficult, no matter how promising the alternative. Tens of millions of people have been trained to write
            and understand SQL, and an enormous volume of business-critical systems rely on it. Replacing SQL not only
            requires migrating software and rewriting queries - it also demands retraining entire workforces, redefining
            best practices, and rebuilding a huge stack of interconnected tools and integrations. Even if a new language
            like Malloy offers clear technical advantages, the switching cost is staggering, and the risks high. </p>
          <p> This is the trap of legacy: the best technology doesn't always win. Sometimes the most compatible and
            entrenched one does. However, platform shifts create rare windows of opportunity - moments when the weight
            of legacy temporarily weakens, and new technologies have a chance to break through and redefine the future.
          </p>
          <h3>Why SQL is not fit for GenAI-Powered Analytics</h3>
          <p> GenAl has the potential to become a primary interface for interacting with data, but it requires
            foundational technologies that are modular, composable, and semantically rich. SQL was never designed for
            this. SQL is a low-level language: every complex analysis must be constructed manually, with little inherent
            support for abstraction or reuse. Metrics are often derived from scratch, based on formulas and business
            rules that live not in the database itself, but in the minds of human experts. These recipes are rarely
            codified in ways that LLMs can easily discover or reuse ‚Äî they are domain-specific, highly contextual, and
            sparsely represented in the training data available to large models. As a result, the risk of hallucination
            and semantic error when generating SQL is extremely high. </p>
          <p> The problem is structural, not stylistic. Writing semantically correct SQL is not simply a matter of
            following grammar rules; it requires correctly reassembling entire analytical concepts from first
            principles, every time. To borrow an analogy from software engineering, SQL is like Assembly language:
            precise, powerful, but fundamentally low-level. An LLM can write syntactically correct Assembly, but it
            becomes vastly more capable when operating in higher-level environments like Python or TypeScript, where
            complex abstractions like networking, storage, and Ul components are already built. Without those
            higher-level building blocks, even trivial applications would require enormous, error-prone efforts. The
            same is true in analytics. Asking LLMs to generate full analyses in SQL - without composable, reusable
            models for key metrics - is akin to asking them to vibe-code an entire web application from raw Assembly.
            It's technically possible, but the room for error grows exponentially with complexity. </p>
          <h3>Malloy was Built for this Moment</h3>
          <p> Malloy was designed with the realities of the GenAl era in mind. It elevates the level of abstraction by
            treating analytic concepts - measures, dimensions, relationships, and queries - as first-class, modular
            building blocks. Instead of forcing an LLM (or a human) to re-derive complex metrics and joins from scratch,
            Malloy allows these analytical patterns to be defined once, in clear, reusable models. This composability
            dramatically reduces the cognitive burden on either man or machine, and minimizes the risk of errors and
            hallucinations when generating analysis. Just as high-level programming languages unlocked new capabilities
            for software developers, Malloy unlocks a higher-level interface for analytics, where GenAl systems can
            build on solid foundations instead of reinventing them every time. In a world where machines are becoming
            active participants in data exploration and decision-making, a modular, semantic, and composable query
            language isn't just helpful ‚Äî it's essential. </p>
          <p> Even in a future where GenAl tools generate much of the initial analysis, human judgment remains
            essential. Analysts, decision-makers, and domain experts will still need to review, modify, and iterate on
            the outputs. However, SQL makes this human-in-the-loop process difficult and error-prone. SQL queries,
            whether written by a human or an LLM, are very often extremely complex and brittle, and challenging to read,
            debug, or safely modify. It's rarely obvious which parts of a long SQL statement can be edited without
            accidentally breaking joins, filters, or business logic. Small changes can have cascading, unintended
            effects. </p>
          <p> Malloy changes this dynamic fundamentally. Because Malloy introduces higher-level abstractions - modular
            models, defined relationships, reusable measures ‚Äî it provides clear, semantically meaningful structures
            that are easier for humans to understand and edit. Instead of reverse-engineering the intent behind hundreds
            of lines of generated SQL, an analyst can quickly see which models, fields, and metrics are being used, and
            can make targeted, safe adjustments. The iteration loop between human and machine becomes faster, more
            reliable, and far less risky. </p>
          <p>In a GenAl-driven world, success won't just be measured by how well machines generate queries ‚Äî it will be
            measured by how well humans and machines can collaborate. Malloy provides the foundation for that
            collaboration to be safe, scalable, and productive. </p>
          <h3>The Opportunity Ahead</h3>
          <p> When RISC architectures like ARM powered the rise of smartphones, it didn't just create better computers -
            it created a revolution in accessibility. Computing, once confined to those who could afford a desktop
            machine, became truly global. Billions of people could suddenly access the internet, applications, and
            information from the palms of their hands. The impact wasn't just technical; it was economic, cultural, and
            societal. </p>
          <p> Today, a similar opportunity exists in analytics. SQL has historically served as the gatekeeper to
            sophisticated data work ‚Äî an essential but specialized skillset, accessible only to those with the time,
            training, and expertise to master it. This bottleneck has limited who can directly interrogate data and
            extract insights. The combination of GenAl and Malloy holds the potential to break that barrier. By making
            analytic logic modular, composable, and semantically rich, Malloy enables LLMs to build, extend, and adapt
            sophisticated analyses - and enables humans without deep SQL expertise to participate meaningfully in the
            process. </p>
          <p> This shift could massively expand the number of people who can engage with data at a sophisticated level.
            Analysts, product managers, operations teams, and even executives could move from passive consumers of
            dashboards to active participants in data-driven problem solving. Just as the smartphone era democratized
            access to computing, the GenAl era - powered by Malloy - has the potential to democratize access to deep
            analytics. </p>
        </div>
      </article>
      <article>
        <label for="holy-grail-data-engineering" class="post-title">Reusability in Data Engineering: Holy Grail or
          Fool's Errand?</label>
        <a href="?postid=holy-grail-data-engineering#blog" class="selflink">[link]</a>
        <time datetime="2024-11-01">11.03.2024</time>
        <input type="checkbox" id="holy-grail-data-engineering" />
        <div class="post-body">
          <p>It‚Äôs not a controversial statement to say that Data Engineering as a discipline does not have much in the
            way of reusability. Maxime Beauchemin explored this idea in his <a
              href="https://preset.io/blog/why-data-teams-keep-reinventing-the-wheel/">recent blog post</a>, and
            attributed the lack of reusability to subtle differences between organizations:</p>
          <blockquote>
            <p> ‚ÄúAs you dig deeper, you realize that no two companies calculate their metrics in exactly the same way.
              Whether it‚Äôs the way they define engagement or measure churn, every business has its own quirks. These
              small but significant differences prevent the kind of standardization and reuse that should be possible in
              software engineering.‚Äù </p>
          </blockquote>
          <p> But in the same way that no two companies are alike, the same can be said of software products. Every
            software product is unique, and yet software engineers across the infinite variety of products can share and
            reuse code libraries for foundational tasks like database management, user authentication and cryptography.
            This reuse allows engineers to focus on building product-specific features rather than reinventing basic
            components. So why doesn‚Äôt data engineering have similar shared core libraries? I believe the problem is not
            the subtle differences between how companies define metrics, but something much more crude and overt: <b>the
              primary language of data engineering (SQL) simply does not allow for it!</b></p>
          <p> First, let‚Äôs define data engineering. It‚Äôs a term that can mean many things, but in this case, I‚Äôm going
            to look at a very SQL-centric view. Let‚Äôs assume data arrives in the warehouse via some load process ‚Äì
            either CDC from a transactional DB, or from a vendor like Fivetran. In this scenario, I define data
            engineering as the process of turning these raw tables (which are often shaped in ways that are more
            suitable for an application to work with) into new tables that are easier for human consumers to use via
            dashboarding tools or hand-written SQL queries. Some might call this ‚Äúanalytics engineering‚Äù. </p>
          <p> One of the most popular tools for this job is <a href="https://www.getdbt.com/">dbt</a>. In dbt, users
            write SQL queries against tables in their data warehouse. These queries form a directed acyclic graph,
            sometimes called a DAG or a pipeline, where each node in the graph is a single query, and the queries are
            all executed in the appropriate order. In these pipelines, each query can be thought of as a function that
            takes one or more tables as input, and produces a single table as output. </p>
          <p>
            <code>
              ùëì(ùë°‚ÇÅ, ùë°‚ÇÇ, ‚Ä¶, ùë°‚Çô) ‚áí ùë°‚Çí
            </code>
          </p>
          <p>Conceptually, this is no different from how functions in standard programming languages work: take some
            input parameters, and produce some output. When writing a function in a language like TypeScript, a software
            engineer might begin by defining an interface for the function‚Äôs input ‚Äì a blueprint that defines which
            parameters are allowed, and their input types. This allows the function to execute on any input data that
            conforms to the required shape. But now consider this scenario in the context of data engineering. In the
            example I described, each of the parameters, ùë°‚ÇÅ through ùë°‚Çô, is a table in the data warehouse, and the
            function, ùëì, is a SQL query. In the first case (software engineer writing TypeScript), the function is
            defined over an abstract interface. Any piece of data that conforms to the interface can be used as input to
            the function. In the latter case (data engineer writing SQL), the function (a SQL query) must be written
            against specific tables that exist in the warehouse, and can only be executed on those tables. There is no
            way in SQL itself to define queries on an abstract interface.</p>
          <p> Let‚Äôs look at one of the simplest possible examples: deduplicating log events. A data warehouse might have
            several tables that contain logged events. Log event tables often contain duplicates due to issues that
            arise in the source systems that emit the events, and data engineers need to deduplicate rows from these
            tables in the warehouse. Usually this looks like a query that performs a <code>row_number</code> window
            function partitioned over the event‚Äôs primary key, and grabs the first row from each window. </p>
          <p>
            <code>
              <pre>
WITH ranked AS (
  SELECT
    *
    , ROW_NUMBER() OVER (
      PARTITION BY event_uuid ORDER BY unixtime ASC
    ) as row_rank
  FROM log_table
)
SELECT *, except(row_rank) FROM ranked
WHERE row_rank = 1
              </pre>
            </code>
          </p>
          <p> I should be able to specify this query by calling a simple ‚Äúdeduplicate‚Äù function, and providing it with
            three parameters: tablename, partition columns, and ordering. Instead, for every table, I need to rewrite
            the full query, despite the fact that the majority of the logic is not unique. </p>
          <p> Many software and data engineers have noticed this problem and end up solving this problem by writing code
            that writes SQL. Often this looks like writing a Python function that wraps a SQL string template and
            produces the desired query based on the function‚Äôs input parameters. Unfortunately, this pushes us into a
            new realm of complexity. The SQL now lives as a string in a Python program which is painful to write and
            maintain, since typical IDE functionality like type-checking and autocomplete won‚Äôt work inside the string.
            Additionally, syntax errors aren‚Äôt caught until runtime, making debugging more difficult. </p>
          <p> From the query runner‚Äôs perspective, the logic is now split between two different languages. Some of the
            business logic lives in SQL, and some lives in Python. Directly executing the query is no longer an option ‚Äì
            first it must be run through a compilation process, and in the event of problems, there are now two separate
            systems that must be debugged: the query generation logic, and the business logic of the query itself. This
            added complexity is significant, and in practice, means that writing these kinds of abstracted data
            pipelines is not accessible to the typical data engineer. </p>
          <p> Taking a step back, this is rather absurd. Imagine if this same idea were true of software engineering. It
            would be as if in order to write reusable Javascript, an engineer would need to wrap it inside of a string
            template in a Python program. It would also mean that only the most talented software engineers could
            actually write reusable components. I hope that one day, data engineers will have a way to write ‚Äúnatively‚Äù
            reusable code, i.e., plain queries that can be run on abstract interfaces, without requiring the use of
            SQL-generating templates. This could happen if SQL adopts a way to run abstract queries (e.g., <a
              href="https://duckdb.org/2024/03/01/sql-gymnastics.html">DuckDB‚Äôs Macros</a>), or some other language or
            framework offers this capability and becomes mainstream. But I firmly believe that until this happens, the
            discipline of Data Engineering will not meaningfully advance. </p>
        </div>
      </article>
      <article>
        <label for="data-modeling-divide" class="post-title">The Data Modeling Divide</label>
        <a href="?postid=data-modeling-divide#blog" class="selflink">[link]</a>
        <time datetime="2023-06-03">06.03.2023</time>
        <input type="checkbox" id="data-modeling-divide" />
        <div class="post-body">
          <p> There is no shortage of opinions on the Internet about how best to organize data: <a
              href="https://en.wikipedia.org/wiki/Star_schema">star schemas</a>, <a
              href="https://www.fivetran.com/blog/star-schema-vs-obt">OBTs</a> (‚ÄúOne Big Table‚Äù), <a
              href="https://en.wikipedia.org/wiki/Slowly_changing_dimension">slowly changing dimensions</a>, and <a
              href="https://www.activityschema.com/">Activity Schema</a>, just to name a few. However, despite the
            existence of all these ‚Äúbest practices‚Äù, most data practitioners I know still describe their data warehouse
            as a complete mess. The most innocent question of ‚Äúwhy are these two numbers different?‚Äù can send an analyst
            or data engineer down a deep rabbit hole for hours or days. With so much discourse about data modeling, why
            are we still in such a fragile situation? In this post, I argue that the root cause is the unnatural
            division of ‚Äúdata modeling‚Äù into two separate workflows: transformation and semantic modeling. </p>
          <p> It's worth taking a second to discuss what data modeling actually is. Here‚Äôs my best attempt at a
            definition: <em>‚ÄúThe process of turning raw data into useful business insights.‚Äù</em> While this definition
            is broad and perhaps a bit too generalized, I‚Äôve noticed that much of the online discourse seems to focus on
            specific tools or techniques. Some people talk about data modeling in terms of the artifacts it produces,
            for example, the aforementioned star schemas and OBTs. Others talk about data modeling as a way to map
            already-existing database objects to business metrics ‚Äì what tables and columns feed into the revenue
            calculation? What segments can we cut revenue by? These two viewpoints correspond to two related, but mostly
            separate camps: data transformation and semantic modeling. Data transformation uses tools like <a
              href="https://www.getdbt.com/">dbt</a> to define pipelines of SQL queries. These queries take raw data
            from business systems as input, and produce ‚Äúclean‚Äù tables and views for downstream consumers to use. In the
            other camp, semantic modeling uses tools like <a
              href="https://cloud.google.com/looker/docs/what-is-lookml">LookML</a> to specify joins between tables and
            define how measures are calculated from those tables. </p>
          <p> A very common workflow is to use dbt to create pipelines that produce clean output tables, then use those
            clean output tables to generate metrics for reporting. These metrics can be defined either in a semantic
            modeling layer like LookML or directly in a visualization tool like Tableau. In either case, it‚Äôs rare for
            the transformation layer to own 100% of the metric logic. There are usually too many dimensional cuts and
            edge cases to enumerate explicitly in the pipeline. </p>
          <p> At first glance this might seem like a reasonable approach, but let‚Äôs look at what happens when the
            inevitable question arises: ‚Äúwhy is Number X different from Number Y?‚Äù Numbers X and Y probably come from
            two different dashboards, and if you‚Äôre lucky enough to use a tool like Looker, you can easily inspect the
            semantic layer to find the measure definitions that generated them. This is great, until you hit a wall:
            measures X and Y are built on tables that are ‚Äúclean output tables‚Äù from a dbt job. Looker has no idea where
            those tables came from or how they were calculated; it just sees the materialized tables in the database. To
            understand the logic behind the columns where measures X and Y come from, you need to jump into dbt and
            trace the pipeline that generated their input tables. There is no longer a single place to look up how a
            metric is defined ‚Äì the business logic for numbers X and Y is split across two tools that use different
            languages (SQL and LookML), and might not even be owned by the same team (BI team owns the BI tool, data
            engineering team owns the transformation tool). If this is the case, good luck in your quest. Crossing team
            boundaries can mean adding hours or even days to a task. </p>
          <p> Transformation and semantic modeling are not fundamentally different tasks ‚Äì they are two parts of the
            same process: turning raw data into business insight. And yet, our ecosystem forces them to happen in
            separate places, oftentimes with tools operated by separate users. </p>
          <p> This problem has been recognized by many in the industry, but existing solutions all fall short. Looker‚Äôs
            Persistent Derived Table functionality allows users to construct data pipelines to materialize their models,
            but practically speaking, the resulting tables are really only usable by Looker, and are difficult to query
            directly from the data warehouse. dbtLabs‚Äô acquisition of Transform is a sign that they have an intuitive
            understanding of the issue, but I‚Äôm not confident that their current direction will improve the situation.
            While full details of the integration haven‚Äôt been published, early indications are that it will look very
            similar to the workflow described above with all its flaws: SQL+dbt for transformations, and a YAML-based
            configuration language for semantic modeling. Still two separate languages and two separate toolsets for
            what is essentially the same problem. </p>
          <p> Is there any solution to all of this? I can‚Äôt say for sure, but in my dream world, transformations don‚Äôt
            create tables or views, they create Models. Models are simply tables with additional metadata attached:
            joins, dimensions, measures, and more. Even further, every intermediate node in a pipeline DAG should also
            be a Model with the same types of metadata attached. Every Model should be queryable by BI tools,
            operational applications, or human analysts using a query language that understands the Model metadata. In
            such a world, the artificial divide between transformation and semantic modeling no longer exists, two tools
            get integrated into one, and the question of ‚Äúwhy are these numbers different‚Äù becomes a lot easier to
            answer. </p>
          <br />
          <p><em>Discuss this post on <a href="https://news.ycombinator.com/item?id=36177068">Hacker News</a></em></p>
        </div>
      </article>
      <article>
        <label for="semantic-layer" class="post-title">What Happened to the Semantic Layer?</label>
        <a href="?postid=semantic-layer#blog" class="selflink">[link]</a>
        <time datetime="2023-04-26">04.26.2023</time>
        <input type="checkbox" id="semantic-layer" />
        <div class="post-body">
          <p>
            <i>Disclaimer: this blog post relates directly to my work as a Product Manager at Google; however, all
              opinions expressed here are my own and do not reflect those of Google. I also can't comment on any future
              Google product plans.</i>
          </p>
          <p> Semantic layers, also known as metrics layers or ‚Äúheadless BI‚Äù, have become a popular topic in recent
            years. The idea of a semantic layer is straightforward but powerful ‚Äì put simply, it‚Äôs a system that takes
            tables in a SQL database and provides business context around them. A semantic layer can help answer
            surprisingly complex questions like ‚Äúwhat is the exact definition of revenue?‚Äù Such an innocent question
            might require a lot of background knowledge to answer: what tables contribute to revenue, and how are they
            joined together? Which columns are the right ones to use, and which rows must be filtered out? It‚Äôs the job
            of a semantic layer to store all of this knowledge, so anyone can ask the question ‚Äúwhat is revenue‚Äù, and
            not need to know all the arcane knowledge about the underlying dataset in order to answer it. </p>
          <p> This is an old idea, present in many enterprise ecosystems (e.g., Microsoft, SAP), but in modern cloud
            data warehousing environments, semantic layers are always tightly coupled to a visualization layer. The BI
            tool <a href="https://www.looker.com/">Looker</a>, with its LookML configuration language is a prime
            example. Much of the recent buzz around this topic relates to decoupling the semantic layer from the
            visualization layer, which would allow many other types of applications to leverage centrally defined
            metrics. Example use cases include customer segmentation for marketing campaigns and lead-scoring in a CRM.
            Benn Stancil called the semantic layer <a href="https://benn.substack.com/p/metrics-layer"> ‚Äúthe missing
              piece of the modern data stack‚Äù</a>. In a post titled <a
              href="https://basecase.vc/blog/headless-bi">Headless Business Intelligence</a>, Ankur Goyal and Alana
            Anderson predicted that it would be the foundation of ‚Äúthe next generation of software companies‚Äú. Airbnb
            made a big splash with a blog post describing its internal metrics layer, <a
              href="https://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70">Minerva</a>.
            All these articles generated a fair amount of excitement in the online data community, but despite all that,
            we have yet to see this supposedly transformative idea change our profession. Several startups gave it a
            shot (<a href="https://transform.co/">Transform</a>, <a
              href="https://www.businesswire.com/news/home/20211109005713/en/Supergrain-Raises-6.8M-from-Benchmark-to-Build-Headless-Business-Intelligence-Platform">Supergrain</a>,
            <a href="https://cube.dev/">Cube</a>, and more), but haven‚Äôt seen much traction amongst analysts. Transform
            was recently acquired by dbt Labs for an undisclosed amount, Supergrain has pivoted, and Cube explicitly
            targets developers rather than analysts. So why has a concept that seemed to resonate so deeply with so many
            in the data community failed to take off? </p>
          <h3>Querying a Semantic Layer</h3>
          <p> To begin, I‚Äôll defer again to Benn Stancil, co-founder and CTO of Mode Analytics, when he asks <a
              href="https://benn.substack.com/p/minerva-metrics-layer">‚ÄúIs Minerva the answer?‚Äù</a> </p>
          <blockquote>
            <p>My biggest gripe with Minerva is how you ‚Äúquery‚Äù it. As best I can tell, metrics are extracted from
              Minerva via API; [...] it looks like the API lets people request metrics over date ranges, with filters,
              and grouped by different dimensions. </p>
            <p> [...] API-based solutions don‚Äôt work for analysts working directly in SQL. Though I don‚Äôt know how
              analysts work at Airbnb, in the broader market, most analysts‚Äô work starts in SQL; for many, it also ends
              there. Unfortunately, Minerva appears to be inaccessible to these workflows. Analysts, when inevitably
              asked to explain why some KPI is yo-yoing across a Minerva-backed dashboard, will eventually have to
              recreate that metric, in SQL, on raw data in their warehouse. </p>
          </blockquote>
          <p> Benn hits it right on the nose. Querying these models takes an analyst out of their most familiar tool
            (SQL) and forces them to use an API, shunting them into tools like Python or R to carry their analysis
            further. If they hit a snag (e.g., numbers don't match, or things look wonky), they're forced back into the
            world of raw SQL without any of the benefits of the metrics layer, and stuck with the unpleasant task of
            resolving differences between hand-written and machine-generated SQL. But even assuming the happy path, once
            in Python-land the analyst is hamstrung in the following ways:
          <ul>
            <li> Analytical freedom is severely limited. Since data from the metrics API is already aggregated, there‚Äôs
              no way to drill into specific records, or create on-the-fly dimensions and measures to slice the data in a
              way that might reveal new insight. You can request other dimensions from the metrics layer, but what if
              the dimension you‚Äôre interested in doesn‚Äôt exist yet? </li>
            <li> The analyst is now constrained to whatever environment is hosting their Python runtime, whether that‚Äôs
              a local laptop or a cloud-hosted notebook. Operations that require more memory or compute aren‚Äôt possible.
            </li>
          </ul> Clients of these metrics layers are isolated from the data warehouse, and only able to communicate with
          the warehouse via a very restrictive API. This might be sufficient for a subset of an analyst‚Äôs work such as
          building executive reports and dashboards, but it falls flat when the analyst needs to do critical work like
          debugging misbehaving metrics, or more creative data work such as exploratory deep-dives. </p>
          <h3>Model Development</h3>
          <p> We‚Äôve looked at the metrics layer from the perspective of an analyst, but what about a data modeler?
            Suppose you want to define a new metric or dimension‚Ä¶ what does the workflow look like? I imagine something
            like the following:
          <ol>
            <li>Write ad-hoc SQL queries against the data warehouse to develop the metric</li>
            <li>Once the SQL logic has been validated, edit a YAML configuration file to encode this logic in the
              semantic layer </li>
            <li> Push the new configuration to source control, potentially get it code reviewed </li>
            <li> Redeploy the semantic layer service, or trigger data pipelines to calculate the new metric </li>
            <li> Get a coffee </li>
            <li> Fire an API call to the semantic layer to get your metric, and confirm that it‚Äôs actually what you want
            </li>
          </ol> Steps 2-5 have the <a
            href="https://medium.com/airbnb-engineering/airbnb-metric-computation-with-minerva-part-2-9afe6695b486#:~:text=Staging%20environment%20where%2C-,within%20a%20few%20hours%2C,-the%20entire%20history">potential
            to take hours</a>, and step 6 takes the analyst away from their most familiar language. If there‚Äôs a bug in
          the metric logic, or something else in the process goes wrong, iterating is painfully slow and requires
          context switching between SQL, YAML, and API. In summary, in order to solve the problem of consistent business
          logic, semantic layers like Minerva take an analyst‚Äôs primary tool (SQL) and break it out into three parts
          that don‚Äôt play well together: SQL for exploration, a modeling language for definitions (typically YAML), and
          an API to interact with the data model. </p>
          <h3>Enter Malloy</h3>
          <p> I recently joined a team at Google trying to solve these issues. We‚Äôre building a new open-source language
            that offers a better way, and it‚Äôs called <a href="https://malloydata.github.io/documentation/">Malloy</a>.
            Malloy unifies the semantic layer and the query language. Instead of writing exploratory SQL, developing
            YAML config, then making API calls, you simply write and execute Malloy. Malloy compiles to SQL, which runs
            directly on the data warehouse. <b> This gives you the superpowers of a semantic layer, without sacrificing
              the freedom to explore, manipulate, and drill into the full unaggregated dataset. </b> </p>
          <p> To see an example that illustrates the point, check out this <a
              href="https://github.dev/malloydata/quickstart/blob/5c6c1ad02c5e799cf17c40f04f3bc5156782ecc7/is_early.malloynb">interactive
              Malloy notebook</a>. It runs directly in your browser in Github's browser-based VSCode environment,
            github.dev. Once your VSCode environment has loaded in your browser, install the Malloy extension, and
            navigate back to the <a
              href="https://github.dev/malloydata/quickstart/blob/5c6c1ad02c5e799cf17c40f04f3bc5156782ecc7/is_early.malloynb">notebook
              file</a>. The notebook demonstrates an example of building up a data model, using that model to explore
            and drill down into the raw data, and using insights from that exploration to iterate further on the data
            model. This sort of workflow is only possible when the semantic layer and the query language are the same,
            unified experience.</p>
          <p> At first glance, it may seem like an impossible task to convince people to learn a new query language.
            After all, SQL has been around for over 40 years, and it‚Äôs ‚Äúgood enough‚Äù, right? Wrong. The excitement
            around metric layers struck a nerve because SQL is clearly NOT good enough. However, most iterations of
            metrics layers up until this point have provided a solution with too grave a cost. They completely fragment
            the analyst workflow and ask users to learn not one, but TWO additional languages: the metric configuration
            language and the metric API (an API is not so different from a small language, after all). With all that in
            mind, it‚Äôs not surprising that the metrics layer hasn‚Äôt taken off. </p>
          <p> This is why I‚Äôm so excited about Malloy. It fulfills the promise of the metrics layer at a fraction of the
            cost. Plus, if we‚Äôre going to ask users to learn a new language, we can seize the opportunity to <a
              href="https://medium.com/@michaeltoy/designing-malloy-0-introduction-88b8809d75d0">design it
              thoughtfully</a>, incorporating everything we‚Äôve learned from the past 40 years of working with data.
            After all, language is a tool for thought, and improving the syntax of a language has deep implications for
            what we can imagine and what we can express. </p>
          <p> If you‚Äôd like to learn more about Malloy, check out the <a
              href="https://github.com/malloydata/quickstart">Quickstart repository</a> and our <a
              href="https://malloydata.github.io/documentation/index.html">documentation</a>. We also have a <a
              href="https://join.slack.com/t/malloy-community/shared_invite/zt-1t32mufpy-THwP1o1ADJVkd3o2L2zaZw">community
              Slack channel</a>, where you are welcome to post any questions or feedback! </p>
        </div>
      </article>
      <article>
        <label for="malloy-tpcds" class="post-title">Exploring the TPC-DS Benchmark Queries with Malloy</label>
        <a href="?postid=malloy-tpcds#blog" class="selflink">[link]</a>
        <time datetime="2023-02-24">02.24.2023</time>
        <input type="checkbox" id="malloy-tpcds" />
        <div class="post-body">
          <p> I‚Äôve been writing a lot recently about <a href="https://github.com/malloydata/malloy">Malloy</a>, an
            experimental analytical query language built by members of Looker‚Äôs founding team. Upon reading the overview
            materials and documentation, it sounded like exactly what I was hoping for, but in order to develop a more
            informed opinion, I needed to get some experience with actually writing the language. Hence, I decided to
            embark on a project to translate each of the 99 TPC-DS benchmark SQL queries to Malloy. This post will give
            an overview of the TPC-DS dataset, the queries, and the opinions I've formed about Malloy along the way. For
            a more introductory overview of Malloy, check out my <a href="?postid=malloy-intro#blog">prior blog
              post</a>, as well as the <a href="https://malloydata.github.io/documentation/">Malloy documentation</a>.
          </p>
          <p> I posted the translations of all 99 queries to <a
              href="https://github.dev/carlineng/malloy-tpcds/tree/remote_files">this Github repository</a>. If you'd
            like to try any of these out for yourself, it's trivial from within the github.dev web editor. Simply go to
            "Extensions" in the lefthand sidebar of the editor and install the Malloy extension. The Malloy queries will
            then be runnable directly in your browser by clicking on the "Run" button above any query. Give it a shot
            with <a
              href="https://github.dev/carlineng/malloy-tpcds/blob/82650a9e105e95b58a14aee1dcfabce3912d2858/malloy_queries/01.malloy#L24">this
              query!</a></p>
          <h2>A Brief Overview of the TPC-DS Benchmark</h2>
          <p> TPC-DS is a well-known standard for benchmarking the performance of OLAP database systems. From a paper
            entitled <a href="https://www.tpc.org/tpcds/presentations/the_making_of_tpcds.pdf">‚ÄúThe Making of
              TPC-DS‚Äù</a>:
          <blockquote> ‚ÄúTPC-DS models the decision support functions of a retail product supplier. The supporting schema
            contains vital business information such as customer, order, and product data. The imaginary retail company
            sells goods through the three distribution channels, store, catalog and Internet (web).‚Äù </blockquote> The
          benchmark contains 99 SQL queries of varying complexity against a star-schema data model containing 7 fact
          tables and 17 dimension tables. This sample schema diagram describing a portion of the data model has been
          taken from the paper above: </p>
          <a href="#img-tpcds-schema"><img loading="lazy" alt="" id="tpcds-schema" src="./img/tpcds_schema.png"
              style="max-width: 50%" /></a>
          <p> The queries test a wide range of OLAP query functionality as specified by the SQL99 standard, including
            common table expressions, ranking window functions, and joins of all types. Because of this, it is
            considered by many to be a ‚Äúcompleteness‚Äù test for a query engine‚Äôs capabilities. For example, as of
            February 2023, <a
              href="https://github.com/Altinity/tpc-ds/tree/bc9725416f96bccc43ad377462768343fe0b3703">Altinity‚Äôs
              benchmark repository</a> reports that Clickhouse currently only passes 75% of the queries. Based on this
            reputation, I thought the query suite would make a great case study in testing the capabilities of Malloy,
            as well as giving me the opportunity to develop a firsthand perspective on the language. </p>
          <h2>Getting Acquainted with the TPC-DS Queries</h2>
          <p> Though I had previously read a lot of TPC-DS benchmark reports and looked superficially at some of the
            queries, I didn‚Äôt actually know the queries or the schema all that well. It didn‚Äôt take long for me to
            realize that the benchmark is far from a picture-perfect dataset with cleanly written SQL. Some of the
            queries contain bugs, the data model is not always applied consistently, and the SQL style employed by the
            authors is somewhat unconventional. </p>
          <h3>Bugs in Queries</h3>
          <p> Several of the queries contain exploding joins due to poorly specified join conditions between tables. In
            some queries, like <a
              href="https://github.com/duckdb/duckdb/blob/cdd23d5e03b2a82b7e611c40a543303722071465/extension/tpcds/dsdgen/queries/02.sql">Query
              2</a>, the results contain duplicate rows that provide no meaningful information. In another example, <a
              href="https://github.com/duckdb/duckdb/blob/cdd23d5e03b2a82b7e611c40a543303722071465/extension/tpcds/dsdgen/queries/54.sql#L26-L33">Query
              54</a>, one of the joins (<code>store</code> and <code>customer_address</code>) causes a fan-out followed
            by a sum on duplicate rows, ultimately generating an incorrect result. In <a
              href="https://github.com/duckdb/duckdb/blob/cdd23d5e03b2a82b7e611c40a543303722071465/extension/tpcds/dsdgen/queries/77.sql#L76-L82">Query
              77</a>, one of the joins is simply missing any join conditions at all, which results in an aggregation on
            top of a cartesian join, again producing incorrect results.</p>
          <h3>Inconsistencies in Data Model Between Queries</h3>
          <p>In <a
              href="https://github.com/duckdb/duckdb/blob/cdd23d5e03b2a82b7e611c40a543303722071465/extension/tpcds/dsdgen/queries/49.sql#L26-L27">Query
              49</a>, the join condition between the <code>store_sales</code> and <code>store_returns</code> tables uses
            two columns, <code>item_sk</code> and <code>ticket_number</code>. In <a
              href="https://github.com/duckdb/duckdb/blob/cdd23d5e03b2a82b7e611c40a543303722071465/extension/tpcds/dsdgen/queries/17.sql#L28-L30">Query
              17</a>, the join between the tables includes a third column, <code>customer_sk</code>. It‚Äôs possible that
            there‚Äôs a legitimate reason why one query should use a different set of join keys than another, but this is
            exactly the sort of arcane knowledge that is likely to cause bugs when working in a complex data warehouse.
            When encountering these tables for the first time, how does an analyst know which join keys are appropriate
            for which situation?</p>
          <h3>Other Miscellaneous Issues</h3>
          <p><b>Join Specification</b>: The majority of joins are specified in the <code>WHERE</code> clause instead of
            using the <code>JOIN</code> keyword. I dislike this syntax because I think it makes queries harder to
            interpret. I prefer to structure my SQL queries such that each part of the query is associated with a
            particular ‚Äúaction‚Äù, e.g., joining, grouping, aggregating, or filtering. Using the <code>WHERE</code> clause
            to specify joins means that filtering and joining operations can be interleaved in the query text, and that
            the join condition is oftentimes far away from the names of the joining tables (see <a
              href="https://github.com/duckdb/duckdb/blob/cdd23d5e03b2a82b7e611c40a543303722071465/extension/tpcds/dsdgen/queries/25.sql#L9-L25">Query
              25</a>). This makes understanding the query just a bit more difficult since filters and joins can be hard
            to distinguish from each other. Understanding how the join occurs now requires jumping back and forth
            between different parts of the query. </p>
          <p><b>Correlated Subqueries</b>: A large number of the queries use <a
              href="https://en.wikipedia.org/wiki/Correlated_subquery">correlated subqueries</a> (e.g., <a
              href="https://github.com/duckdb/duckdb/blob/cdd23d5e03b2a82b7e611c40a543303722071465/extension/tpcds/dsdgen/queries/01.sql#L15-L18">Query
              1</a> and <a
              href="https://github.com/duckdb/duckdb/blob/cdd23d5e03b2a82b7e611c40a543303722071465/extension/tpcds/dsdgen/queries/41.sql#L5-L7">Query
              41</a>). I‚Äôve always found correlated subqueries hard to understand, since the join syntax requires
            joining a column in the subquery to an ‚Äúoutside‚Äù column, which oftentimes lacks immediate context to
            indicate where it comes from. This forces the reader to backtrack (oftentimes quite far) to earlier in the
            query to figure out what‚Äôs going on. </p>
          <p><b>GROUP BY ROLLUP</b>: A lot of the queries use the <code>GROUP BY ROLLUP</code> construct (e.g., <a
              href="https://github.com/duckdb/duckdb/blob/cdd23d5e03b2a82b7e611c40a543303722071465/extension/tpcds/dsdgen/queries/18.sql#L41-L44">Query
              18</a>). While most modern data warehouses support this functionality, I‚Äôve very rarely seen it used in
            practice because the output is so clumsy and difficult to work with. For more on this topic, check out my <a
              href="https://carlineng.com/?postid=sql-bad-syntax#blog">prior blog post</a>. </p>
          <p>To be fair, most of the above issues are quite likely to appear in a real-world data warehouse. It‚Äôs not
            unreasonable for a benchmark suite to contain problems like these, but I doubt they were intentional. I
            couldn‚Äôt find any mention of intentionally including mistakes when I browsed through the TPC-DS reference
            materials. </p>
          <h2>Translating to Malloy</h2>
          <p> The Malloy language currently supports querying Postgres, DuckDB, and BigQuery. I opted to use DuckDB
            because of the ease of setup ‚Äî it‚Äôs an in-process database that doesn‚Äôt require running a separate server or
            a connection to the Internet, and it operates directly on local files.</p>
          <p> Once I had the database up and running, writing the actual queries was pretty straightforward. The
            examples in the Malloy documentation provide a great template for getting started. I created a <a
              href="https://github.dev/carlineng/malloy-tpcds/blob/82650a9e105e95b58a14aee1dcfabce3912d2858/malloy_queries/tpcds.malloy">‚Äúmodel
              file‚Äù</a> to develop the data model with predefined joins, commonly used aggregations, and other reusable
            constructs. Each individual query file imports this model file and uses one of the Sources defined there as
            a starting point for querying. Of the 99 queries, I was able to write logical translations of all but one:
            <a
              href="https://github.dev/carlineng/malloy-tpcds/blob/82650a9e105e95b58a14aee1dcfabce3912d2858/malloy_queries/51.malloy">Query
              51</a>, which requires cumulative window functions. Of the 98 translated queries, 96 returned successfully
            with correct answers. One query failed due to a bug where the Malloy-to-SQL compilation process generated
            invalid SQL (<a
              href="https://github.dev/carlineng/malloy-tpcds/blob/82650a9e105e95b58a14aee1dcfabce3912d2858/malloy_queries/14.malloy">Query
              14</a>), and the other was due to DuckDB crashing on the compiled SQL (<a
              href="https://github.dev/carlineng/malloy-tpcds/blob/82650a9e105e95b58a14aee1dcfabce3912d2858/malloy_queries/64.malloy">Query
              64</a>). Given that the TPC-DS queries are considered a ‚Äúcompleteness‚Äù benchmark for logical capabilities,
            I was rather impressed with Malloy‚Äôs coverage. 97% (96 out of 99) is solid, especially for a project so
            young. </p>
          <p> In total, the Malloy queries consisted of 4,727 lines, 13,788 words, 129,880 characters. The SQL queries
            from the DuckDB repository are 5,524 lines, 14,961 words, and 184,424 characters. In lines of code, the
            Malloy is about 15% more compact, and over 30% more compact when looking at character count. In addition to
            being more concise, I‚Äôve found the Malloy queries to be quite a bit more readable. <a
              href="https://github.com/duckdb/duckdb/blob/cdd23d5e03b2a82b7e611c40a543303722071465/extension/tpcds/dsdgen/queries/04.sql">Query
              4</a> is an extreme example -- the SQL query comes in at a whopping 119 lines of code. Parsing a single
            119 line SQL query is a task that requires a good deal of mental focus. The Malloy equivalent is only 37
            lines:</p>
          <div id="tpcds4" class="side-by-side">
            <div class="sbs-child">
              <script src="./gists/tpcds4sql.js"></script>
            </div>
            <div class="sbs-child">
              <script src="./gists/tpcds4malloy.js"></script>
            </div>
          </div>
          <p>It‚Äôs not a completely fair comparison, since the Malloy query leverages pre-defined logic imported from the
            model file, but even with that in mind, I find the difference between the two implementations quite
            informative. </p>
          <p> Overall, writing Malloy queries is a lovely experience. Coming from SQL, the syntactic structure of the
            language feels familiar, but much more refined ‚Äî as if the clunky, jagged edges of the language have been
            reshaped and sanded down, leaving something with a much smaller footprint, and a much more pleasing
            ergonomics. Malloy‚Äôs syntax seems smaller and has a more explicit structure. Unlike SQL, it doesn‚Äôt make the
            hopeless attempt to mimic human language, and seems instead optimized for someone trying to manipulate data
            using a well-understood set of actions ‚Äî grouping, joining, aggregating, and filtering. </p>
          <p>Interestingly, the explicit structure imposed by the language meant that I settled in very quickly on
            conventions for formatting my Malloy queries. When writing a query, there are far fewer formatting decision
            points than in the equivalent SQL query. While this may seem trivial at first, it‚Äôs a subject that has
            spawned a <a href="https://github.com/un-ts/prettier/tree/master/packages/sql">number</a> of <a
              href="https://sqlfum.pt/">linting</a> <a href="https://github.com/sqlfluff/sqlfluff">packages</a>, and
            more than a few <a href="https://benn.substack.com/p/the-case-against-sql-formatting">essays by analytics
              influencers</a>. Reducing the time spent picking apart query formatting means more mental energy can be
            spent on the logic itself. This is a much improved experience for writing queries, and even better for
            reading them. There are countless examples of this in the TPC-DS queries, but <a
              href="?postid=malloy-tpcds&scrollto=tpcds4#blog">Query 4</a> above is a prime example. The SQL query
            contains filters with nested case statements, <code>UNION ALL</code> statements, and common table
            expressions. With each of these elements, formatting is applied inconsistently, and it took me a good deal
            of time to piece together the logic. The <a href="?postid=malloy-tpcds&scrollto=tpcds4#blog">Malloy
              version</a> is clearly much more concise and far less chaotic.</p>
          <p> Drilling into a few more details, here are some additional points where I found Malloy to be a clear
            improvement over SQL:</p>
          <p><b>Joins:</b> the ability to leverage a pre-defined data model and not have to remember and write out join
            keys for every single join was really great. </p>
          <p><b>Nested Aggregations:</b> Nested aggregations take the place of <code>GROUP BY ROLLUP</code>, and do a
            far better job (see <a
              href="https://github.dev/carlineng/malloy-tpcds/blob/82650a9e105e95b58a14aee1dcfabce3912d2858/malloy_queries/22.malloy">Query
              22</a>). As mentioned previously, see my <a href="?postid=sql-bad-syntax#blog">previous blog post</a> for
            a deeper dive on this topic.</p>
          <p><b>Filtered Aggregations:</b> Filtered aggregates are so much nicer to use and interpret than using CASE
            statements inside aggregate functions (see <a
              href="https://github.dev/carlineng/malloy-tpcds/blob/82650a9e105e95b58a14aee1dcfabce3912d2858/malloy_queries/21.malloy">Query
              21</a>). Some SQL databases have implemented this functionality, and I think it‚Äôs a no-brainer to make it
            a standard part of the language. </p>
          <div id="tpcds21" class="side-by-side">
            <div class="sbs-child">
              <script src="./gists/tpcds21sql.js"></script>
            </div>
            <div class="sbs-child">
              <script src="./gists/tpcds21malloy.js"></script>
            </div>
          </div>
          <h2>Potential Areas for Improvement</h2>
          <p> While the experience was overwhelmingly positive, there were still a few spots where I fell into traps, or
            where the language didn‚Äôt feel quite optimized for the task. </p>
          <p><b>LEFT JOIN vs INNER JOIN:</b> All joins in Malloy are <code>LEFT JOIN</code>s by default. To implement
            <code>INNER JOIN</code> semantics, a query needs to specify a filter on
            <code>right_table.column = null</code>, which I would often forget to do. It‚Äôs trivial to add these filters,
            but in SQL queries, the type of join is a critical piece of information that is oftentimes the first thing
            an analyst will look at to determine the nature of a query. Having the <code>INNER JOIN</code> indicator
            placed in a <code>`where`</code> clause might not be obvious to someone quickly glancing at a query, since
            <code>`where`</code> clauses are typically reserved for filtering down datasets based on properties. From a
            UX perspective, since joins in Malloy are usually defined in a Source and not necessarily the query itself,
            it‚Äôs not clear to me how to make this more obvious, and it might just be one of those things that takes some
            getting used to with a new tool. </p>
          <p>
            <b>SEMI-JOIN:</b> The semi-join operation in SQL takes the form of
            <code>WHERE customer_id IN (SELECT ‚Ä¶ FROM ‚Ä¶)</code>, as seen in <a
              href="https://github.com/duckdb/duckdb/blob/cdd23d5e03b2a82b7e611c40a543303722071465/extension/tpcds/dsdgen/queries/33.sql#L8-L11">Query
              33</a>. The Malloy implementation of this construct requires an extension to a Source with an ad-hoc join
            (lines 2-7 in the query below), and an additional <code>NULL</code> filter in the <code>where</code> block
            (line 16):
          </p>
          <script src="./gists/tpcds33malloy.js"></script>
          <p>Logically, this produces the correct output, but I don‚Äôt love the syntax since it combines two operations
            that could each have separate semantic meanings, and which are far apart in the query. This makes it harder
            to interpret exactly what the query is doing. By contrast, the SQL syntax of
            <code>column IN (sub-select)</code> is neatly contained in a single location, and unambiguous in its intent.
          </p>
          <h2>Conclusion</h2>
          <p>Throughout this entire process, I learned a lot about both Malloy and the TPC-DS benchmark. Initially,
            writing 99 queries in a new query language felt like a huge task, but it wasn‚Äôt long before I hit my stride.
            The Malloy language made the task easy, and dare I say, fun. Getting up close with the benchmark queries was
            hugely educational as well. The dataset and queries are impressive in many aspects, but ultimately, I don‚Äôt
            think they‚Äôre very representative of actual workloads. Even though it has earned a reputation as a coverage
            test for a query engine‚Äôs logical completeness, there are still many real-world query patterns and scenarios
            that are not represented, such as <a href="https://mode.com/blog/finding-user-sessions-sql/">session
              analysis</a>, or the <a
              href="https://discourse.getdbt.com/t/finding-active-days-for-a-subscription-user-account-date-spining/265">construction
              of ‚Äúdate-spined‚Äù reporting tables</a>. As a result, I‚Äôve come away from the exercise more excited about
            Malloy than ever, but acutely aware that there is still much to be tested. Will Malloy ever truly replace
            SQL as the lingua franca of the analytics world? If it does, it will take a long time to get there, but it‚Äôs
            a future I‚Äôd like to live in. </p>
        </div>
      </article>
      <article>
        <label for="sql-renaissance" class="post-title">SQL, Malloy, and the Art of the Renaissance</label>
        <a href="?postid=sql-renaissance#blog" class="selflink">[link]</a>
        <time datetime="2023-02-05">02.05.2023</time>
        <input type="checkbox" id="sql-renaissance" />
        <div class="post-body">
          <p> This post is the third in a series comparing SQL with a promising new query language called <a
              href="https://www.malloydata.dev/">Malloy</a>. I think Malloy represents a leap forward in how we work
            with data, and in the following paragraphs, I'll attempt to draw a connection to another time in history of
            rapid technological advancement: the Renaissance in Western Europe.</p>
          <p>Lloyd Tabb, former Looker CTO and creator of Malloy, recently wrote a post entitled <a
              href="https://lloydtabb.substack.com/p/data-is-rectangular-and-other-limiting">Data is Rectangular and
              Other Limiting Misconceptions</a>. If you haven‚Äôt read it yet, go check it out now ‚Äî it very neatly lays
            out a fundamental flaw in SQL‚Äôs perspective of data, and describes how the Malloy language does better. In
            particular, SQL resultsets can only naturally represent data in two dimensions: rows and columns. However,
            data in the real world is NOT two-dimensional. This mismatch results in awkwardness in both writing SQL
            queries and interpreting SQL results. As I was reading Lloyd's post, it sparked a memory of a recent trip I
            took to the Uffizi art gallery in Florence, Italy. </p>
          <p> The Uffizi is the former administrative office of the powerful Medici family that ruled Florence during
            the 15th century. It currently houses one of the world‚Äôs largest collections of Gothic and Renaissance art,
            spanning from the 12th to the 16th century in Western Europe. I personally have minimal knowledge of art and
            art history, but the tour we took highlighted some fascinating aspects of technological advancement during
            this time. </p>
          <h3>Gothic Art</h3>
          <p> Gothic paintings such as this <a
              href="https://www.uffizi.it/en/artworks/virgin-and-child-enthroned-and-prophets-santa-trinita-maesta">
              depiction of the of the Virgin Mary</a>, attributed to Florentine painter Cimabue circa 1290-1300, were
            flat scenes with close-up figures in the foreground. The notion of perspective had not yet been developed,
            and rather than attempt to create images with depth, many paintings filled the background with gold leaf,
            which served as a representation of heaven.</p>
          <a href="#img-cimabue"><img loading="lazy" alt="" id="renaissance-cimabue" src="./img/cimabue-mary.jpeg"
              style="max-width: 100%; height: 400px" /></a>
          <h3>Transitioning to the Renaissance</h3>
          <p>The transition from the Gothic to the Renaissance period was marked, among other things, by a growing
            understanding of perspective: the ability to accurately represent three-dimensional scenes on a flat canvas.
            <a href="https://www.uffizi.it/en/artworks/battle-of-san-romano">The Battle of San Romano</a> by Paolo
            Uccello circa 1435-1440 is a great example from this transition period. A fierce battle rages in the
            foreground, while scenes from life in the Italian countryside play out in the background. Uccello clearly
            understood that far away objects in the background must be smaller than those in the foreground, but just
            how much smaller hadn‚Äôt been fully worked out yet. Look at the hunting scene in the background ‚Äî the hunters
            and hares would be massively tall if brought to the foreground. </p>
          <a href="#img-uccello"><img loading="lazy" alt="" id="renaissance-uccello"
              src="./img/uccello-battle-of-san-romano.jpeg" style="max-width: 100%" /></a>
          <h3>The Renaissance</h3>
          <p>Finally we arrive at <a href="https://www.uffizi.it/en/artworks/annunciation">The Annunciation by Leonardo
              da Vinci</a>, painted around 1472-1476. Here, the lines of the building behind Mary clearly converge into
            a well-defined <a href="https://en.wikipedia.org/wiki/Vanishing_point">vanishing point</a> in the
            background. The mountains, ships, and trees behind the kneeling archangel Gabriel appear realistically
            distant. This was as much a technological advancement as an artistic one. Da Vinci applied his knowledge of
            mathematics to accurately represent the three-dimensional world on a two-dimensional canvas. </p>
          <a href="#img-davinci"><img loading="lazy" alt="" id="renaissance-davinci"
              src="./img/davinci-annunciation.jpeg" style="max-width: 100%" /></a>
          <h3>SQL: The Gothic Art of Query Languages</h3>
          <p> A SQL resultset is a two-dimensional canvas with rows on one axis and columns on the other. The typical
            star-schema data model contains hundreds of dimensions which can be used to slice, dice, and aggregate data.
            Because a SQL resultset can only naturally represent two dimensions, any single projection of the
            star-schema model can only represent the data in two dimensions. A query that aggregates along a single
            level of granularity is like the Gothic painting above -- a flat scene representing two dimensions, without
            any notion of depth. Queries that require aggregation at different levels of granularity (like the
            hierarchical subtotals of my <a href="index.html?postid=sql-bad-syntax#blog">last post</a>) are possible,
            but fit awkwardly into the two-dimensional output of the query. Looking at the results of a <code>GROUP BY
            ROLLUP</code> query is akin to looking at the Battle of San Romano. The point comes across, but the image
            is contorted and unnatural. </p>
          <h3>Malloy: A Query Language Renaissance?</h3>
          <p> Malloy‚Äôs approach breaks free of the two-dimensional constraints of rows and columns by allowing columns
            to take a ‚Äútable‚Äù type. This means sub-tables can be nested within a single column, and hierarchies can be
            preserved. By allowing sub-tables within resultsets, Malloy results are able to faithfully represent the
            true dimensionality of the underlying data. The idea of nested tables is not a new one. Database researchers
            have discussed this idea of "relation-valued attributes" <a
              href="https://www.sciencedirect.com/science/article/abs/pii/0306437986900037">since at least 1986</a>, but
            no relational database on the market supports them natively. </p>
          <p>I described the following example in depth in my <a href="index.html?postid=sql-bad-syntax#blog">previous
              post</a>, but to quickly go over it again, the following SQL query generates a result aggregated at both
            the yearly and monthly level.</p>
          <script src="./gists/art__group_by_rollup_sql.js"></script>
          <script src="./gists/art__raw_results_with_duplicates.js"></script>
          <p>Note that data aggregated at the yearly level is duplicated in every row. This column is now "unsafe" for
            use by downstream queries. Summing the <code>yearly_sales</code> column will result in values that are
            wildly incorrect.</p>
          <p>Compare this to the equivalent Malloy query:</p>
          <script src="./gists/art__malloy_rollup.js"></script>
          <p>The <code>by_month</code> column contains a nested table, with its own rows and columns, and we no longer
            have duplicated data for yearly sales.</p>
          <a href="#img-malloy-nest-result-2"><img loading="lazy" alt="" id="malloy-nest-result-2"
              src="./img/malloy_nest_result.png" style="max-width: 100%" /></a>
          <p>This is more than just an aesthetic improvement. From a practical standpoint, any Malloy result can be
            plugged into downstream computations without worrying about the necessity of arbitrary filters or falling
            into the <a href="https://stackoverflow.com/questions/14328319/fan-trap-and-chasm-trap-database">fan/chasm
              traps</a>. The importance of this cannot be overstated! Writing a query the "obvious" way in SQL will
            oftentimes result in something that is syntactically valid, but semantically meaningless. I believe this is
            a major deficiency of the tools we use. The "obvious" way should be the correct way, and anything less is a
            serious design flaw.</p>
        </div>
      </article>
      <article>
        <label for="sql-bad-syntax" class="post-title">Why SQL syntax sucks, and why it matters</label>
        <a href="?postid=sql-bad-syntax#blog" class="selflink">[link]</a>
        <time datetime="2022-12-07">12.07.2022</time>
        <input type="checkbox" id="sql-bad-syntax" />
        <div class="post-body">
          <p>In a <a href="https://carlineng.com/?postid=malloy-intro#blog">previous blog post</a>, I wrote about
            Malloy, a new language for querying analytical databases. My main argument was that the killer feature of
            Malloy is its integration of a semantic layer inside an interactive query language. The <a
              href="https://news.ycombinator.com/item?id=32738874">discussion on Hacker News</a> was, as expected, quite
            lively, and provided great food for thought. This post is a response to some of those comments, an
            exploration of some of the deficiencies of SQL's syntax, and a demonstration of how Malloy gets it right.
          </p>
          <p>A lot of those comments were along the lines of: </p>
          <blockquote>
            <p><a href="https://news.ycombinator.com/item?id=32738874#32744949"> SQL + recursive queries is Turing
                complete. Thus every arrangement and selection of data is reachable by a query. Therefore there is
                nothing Malloy can do that SQL can't (I'm absolutely sure of it).</a></p>
          </blockquote> and: <blockquote>
            <p><a href="https://news.ycombinator.com/item?id=32738874#32744378">I don't see much value in this. This is
                not aesthetically better than SQL. It's also not semantically better. This is just a different syntax
                that would parse to the same AST.</a></p>
          </blockquote>
          <p> Both of these comments dismiss Malloy, pointing out that one can write semantically equivalent queries in
            SQL. While they are technically correct that Malloy queries all have semantically equivalent SQL queries,
            this misses a critical point: <b>the syntax of a language has a profound impact on what users of that
              language choose to express.</b> As the saying goes, ‚Äúfirst we shape our tools, then our tools shape us‚Äù.
          </p>
          <p>After over a decade of using SQL nearly every day for data analysis, I‚Äôve developed a strong conviction
            that SQL‚Äôs syntax is not just awkward and annoying, but actively harmful. By making certain types of queries
            difficult to write and interpret, it trains users to avoid asking those questions. In <a
              href="https://news.ycombinator.com/item?id=32738874#32746468">this thread</a>, some commenters take on the
            challenge of implementing a SQL equivalent to a Malloy query. However, I think the relevant question is not
            ‚Äúcan you write a SQL query that answers question X?‚Äù, but rather ‚Äúwould you have thought to ask question X
            in the first place?‚Äù </p>
          <p> This speaks more broadly to a common <a
              href="https://ryxcommar.com/2022/11/27/goodbye-data-science/">dissatisfaction I see amongst analysts and
              data scientists</a>. Many analytics jobs are essentially writing SQL queries to answer questions from
            higher-ups. Data lies at the bottom of a mystical lake, and only the analyst knows how to mutter the right
            SQL incantations to retrieve it. However, when the time comes for the analyst to ask their own questions,
            the first ideas that will likely come to mind are ideas that are easy to express, and not necessarily those
            that are most impactful or creative. I suspect this is especially true in cases where the analyst does not
            have deep domain expertise, and operates in a centralized ‚Äúticket-taking‚Äù support model. The syntax of SQL
            subtly guides a user‚Äôs mindset towards answering particular types of questions, and avoid answering others.
          </p>
          <h3>A Simple Example: Nested Analysis</h3>
          <p>A concrete example of something difficult in SQL is any kind of nested analysis, such as computing
            subtotals and percent of total in a hierarchical dimension. For example, suppose we have a table of daily
            sales, and we want to roll up total sales simultaneously at the monthly and yearly level: </p>
          <script src="./gists/sql_sucks__sales_by_date.js"></script>
          <p> This is typically done in SQL with one of three approaches: (1) computing multiple subqueries and joining
            the results together, (2) using a window function to aggregate at different levels, or (3) using a GROUP BY
            ROLLUP clause, if supported by the database. The queries for each of these approaches is below: </p>
          <script src="./gists/sql_sucks__multiple_grain_queries.js"></script>
          <p> Each of these three options produces a correct answer, but presented in a seriously flawed way. A query
            like this requires the data to be aggregated along multiple grains (year and month, in this case); however,
            a typical SQL query is only capable of naturally representing data in two dimensions (rows and columns),
            which corresponds to one level of granularity. To represent more than one level of granularity, a query must
            resort to unnatural representations that are difficult to interpret and use. This is tough to explain in
            words, but hopefully the following examples are more clear. In approaches (1) and (2) above, the SQL results
            contain duplicate data for the coarser grain (yearly sales):</p>
          <script src="./gists/sql_sucks__results_with_duples.js"></script>
          <p> The <code>yearly_sales</code> column above contains duplicate data for each month's row. Doing a naive
            <code>SUM(yearly_sales)</code> aggregation on this table is a dangerous operation that will lead to
            incorrect results without the appropriate filters.
          <p>In approach (3), yearly aggregates are represented as rows where the finer-grained column (month) contains
            <code>NULL</code> values: </p>
          <script src="./gists/sql_sucks__results_with_nulls.js"></script>
          <p>Values aggregated at different grains are all contained in the <code>total_sales</code> column. Whether
            it's a yearly aggregate or a monthly aggregate depends on the values of other columns. Here again, running
            <code>SUM(total_sales)</code> is potentially a very dangerous operation. </p>
          <p>In all of these cases, the resulting ‚Äúsales‚Äù columns can no longer be used as inputs to aggregate functions
            or other downstream transformations, since the rows contain duplicated information. If this table is
            consumed by downstream queries, those queries must take extreme care to apply the appropriate filters or
            transformations to prevent duplicates from entering the calculations.</p>
          <h3>The Malloy Way</h3>
          <p>The <a href="https://malloydata.github.io/malloy/documentation/language/nesting.html">nesting
              functionality</a> in Malloy allows you to write a subquery within an aggregation. This will produce a
            sub-table for each row in the GROUP BY clause. It feels much more natural to write the query this way, since
            the query is now written in a way that matches the hierarchical logic that‚Äôs being performed:</p>
          <script src="./gists/sql_sucks__malloy_rollup.js"></script>
          <p>The output is presented in a way that respects the hierarchical relationship of the data. Every aggregate
            column contains data at a single level of granularity, and we no longer see duplicate values for data at the
            coarser grain.</p>
          <a href="#img-malloy-nest-result"><img loading="lazy" alt="" id="malloy-nest-result"
              src="./img/malloy_nest_result.png" style="max-width: 100%" /></a>
          <p>This is possible because the <code>by_month</code> column in the result set of this query is of type <a
              href="https://duckdb.org/docs/sql/data_types/struct.html"><code>struct</code></a>, which effectively
            contains a sub-table. The Malloy VSCode extension renders this <code>struct</code> data as if it were just
            another table. This is easier to write, easier to interpret, and avoids all the pitfalls of the equivalent
            SQL implementation. The output of this query can be used naturally by any downstream queries or analyses
            without having to worry about hidden traps.</p>
          <p>It‚Äôs a pretty simple example, but I think it illustrates the point nicely. In my career as a SQL monkey,
            I‚Äôve <b>*been asked*</b> to do analysis like this many times in the past, and its always been possible. But
            when looking at a dataset on my own, its rare that these are the first questions that <b>*I will ask on my
              own.*</b> I‚Äôm convinced that these syntactic improvements offer more than just enhanced productivity for
            the user, but enhanced creativity as well. To see what‚Äôs possible with more sophisticated examples, I highly
            recommend checking out some of the <a
              href="https://malloydata.github.io/malloy/documentation/examples/ecommerce.html">examples on the Malloy
              documentation site</a>, or playing with the <a href="http://malloy.lloydtabb.com/"> Composer application
            </a>, a web app built on top of Malloy. </p>
        </div>
      </article>
      <article>
        <label for="malloy-intro" class="post-title">A Sequel to SQL? An introduction to Malloy</label>
        <a href="?postid=malloy-intro#blog" class="selflink">[link]</a>
        <time datetime="2022-09-05">09.05.2022</time>
        <input type="checkbox" id="malloy-intro" />
        <div class="post-body">
          <p><i>Follow along with the discussion at <a href="https://news.ycombinator.com/item?id=32738874">Hacker
                News</a>.</i></p>
          <p>The foundations of SQL were laid at the dawn of the relational database. Back then, there was no such thing
            as a data warehouse, no such thing as a BI tool, and certainly no such thing as an <a
              href="https://www.getdbt.com/what-is-analytics-engineering/">Analytics Engineer</a>. And yet, SQL is still
            the primary user interface by which most data professionals interact with their raw materials. The
            underlying technologies have improved immeasurably, but aside from a handful of updates to the ANSI
            standard, the core of the language remains untouched. It‚Äôs practically a miracle that after 40+ years of use
            by countless data professionals, our interface to data is effectively the same. </p>
          <p>Since its inception, many computer scientists and database researchers have expressed their disdain for
            SQL, and their critiques are usually well-founded; however, every serious attempt to replace it as the
            de-facto standard has failed. Most attempts to replace SQL primarily address the language‚Äôs awkward syntax;
            for example putting the <code>FROM</code> clause first or removing the need for <code>HAVING</code> and
            <code>QUALIFY</code> clauses. Unfortunately, the reality is that SQL is ‚Äúgood enough‚Äù for most use cases,
            and according to <a href="https://youtu.be/IVlMB9akD1A?t=420">Aiken‚Äôs Law</a>, programmer training is the
            dominant cost for a programming language. It seems that syntactic sugar is simply not enough to overcome
            SQL‚Äôs entrenchment. </p>
          <p>This brings me to <a href="https://github.com/looker-open-source/malloy">Malloy</a>, a new query language
            for data analysis currently being developed by <a href="https://twitter.com/lloydtabb">Lloyd Tabb</a>,
            founder and former CTO of <a href="https://www.looker.com/">Looker</a>. Malloy addresses many of the
            aesthetic concerns that plague SQL, but far more interesting in my opinion is its integration of a query
            language and a semantic layer into a single language. But what is a semantic layer, and why is it important?
          </p>
          <h3>Semantic Layers</h3>
          <p>A semantic layer‚Äôs purpose is to codify domain-specific logic on top of database tables, and to prevent
            users from issuing queries that are syntactically valid, but semantically meaningless. For example, what
            happens if you write a join between two tables with incorrect keys? In most relational databases, primary
            and foreign keys are just strings or numbers. Some databases may enforce referential integrity, but most
            will not complain if you attempt to join, say, <code>CUSTOMER_ID</code> with <code>ORDER_ID</code>. As
            another example, it‚Äôs often the case that primary key columns are integers, and databases will happily let
            you use them as inputs to any aggregate function that takes a number input, like <code>SUM</code> or
            <code>AVG</code>, even though the result is nonsense. Lastly, every organization has special rules that must
            be applied to their datasets in order to correctly calculate key metrics. For example, what inputs and
            adjustments go into calculating the revenue that Investor Relations reports out to Wall Street? Someone with
            access only to the raw data, but not the requisite domain-specific knowledge will not be able to accurately
            reproduce these metrics. The semantic layer provides a place to set these rules and require queries to abide
            by them; e.g., which joins are valid, which columns can be grouped on, or what inputs go into a particular
            aggregate function. </p>
          <p>A semantic layer usually takes the form of an application that sits on top of the database, along with
            configuration files that define the rules described above. Examples of products on the market today include
            SAP‚Äôs <a
              href="https://help.sap.com/docs/SAP_BUSINESSOBJECTS_BUSINESS_INTELLIGENCE_PLATFORM/3d4f417fd0764f909c0ef7931e19fe1a/e4af1c39d2b94ca5bc8a991b4ff26f5f.html?locale=en-US">Business
              Objects Universe</a>, Looker‚Äôs <a href="https://cloud.google.com/looker/docs/what-is-lookml">LookML</a>
            and <a href="https://cube.dev/">Cube</a>. I was an early user of Looker, and that experience left a strong
            impression. LookML allowed me to define the logic necessary to package all of our data sources together as a
            cohesive single source of truth. Our data warehouse turned from a tangled mess of tables that only a skilled
            data scientist could operate, into a trusted repository that the typical PM or business partner could pull
            insights from. </p>
          <p>Despite the huge value we got out of Looker and LookML, the analytics and data science group never loved it
            as much as I did. In comparison with <a href="https://www.tableau.com/">Tableau</a>, Looker‚Äôs interactive
            data exploration and analysis capabilities are relatively limited, and most data scientists saw exploration
            and analysis as their primary activity. They viewed writing LookML configuration as a chore, and the user
            experience did not help. To add a new dimension or measure, a data scientist would have to edit a YAML
            configuration file, check their changes into source control, and reload their configuration before they
            could view any results. A simple workflow that could take 2 seconds in Tableau might take over a minute in
            Looker. When trying to explore a dataset, the ability to iterate ‚Äúat the speed of thought‚Äù is critical, and
            that extra latency was a source of frustration for many. As a result, data modeling and data exploration
            were viewed as two entirely separate disciplines. Data scientists greatly preferred tools that aided the
            latter, much to the detriment of everyone who was NOT a data scientist. </p>
          <h3>Malloy</h3>
          <p>So finally we return to Malloy. As I mentioned previously, Malloy is a query language that compiles to SQL,
            and looks very familiar to anyone who has used SQL. The team has also built a VSCode extension that allows
            users to connect to a database and start writing queries, currently with support for BigQuery, DuckDB and
            Postgres. The semantic layer within Malloy is accessed via writing Sources. From the <a
              href="https://looker-open-source.github.io/malloy/documentation/language/source.html#sources">docs</a>:
          </p>
          <blockquote>
            <p>A source can be thought of as a table and a collection of computations and relationships which are
              relevant to that table. These computations can consist of measures (aggregate functions), dimensions
              (scalar calculations) and query definitions; joins are relationships between sources.</p>
          </blockquote>
          <p> Let‚Äôs take a look at an example Source from the <a
              href="https://github.com/looker-open-source/malloy/blob/2135a51c46f3076f9d28c2136f9d8ca4cf9a101b/samples/duckdb/faa/2_flights.malloy#L8-L32">Malloy
              Github repo</a>, looking at a database of flights.: </p>
          <p><code>
<pre>source: flights is table('duckdb:data/flights.parquet') + {
  primary_key: id2

  // rename some fields as from their physical names
  rename: origin_code is origin
  rename: destination_code is destination

  // join all the data sources
  join_one: carriers with carrier
  join_one: origin is airports with origin_code
  join_one: destination is airports with destination_code
  join_one: aircraft with tail_num

  // declare some resusable aggregate calculations
  measure:
    count is count()
    total_distance is sum(distance)
  }
}</pre></code>
          </p>
          <p>This Source has a few key components: the name of a table in the database, the primary key of the table,
            which tables can be joined to it and via which columns, and which aggregations (aka measures) are valid.
            Within the VSCode extension, I can use this source as the starting point for data exploration or analysis
            simply by writing a query referencing it, and hitting the ‚ÄúRun‚Äù CodeLens button: </p>
          <p>
            <a href="#img-malloy-src"><img loading="lazy" alt="" id="malloy-src" src="./img/malloy_src.png"
                style="max-width: 100%" /></a>
          </p>
          <p>The VSCode extension compiles the Malloy query to SQL, issues it against the database, and renders the
            results in another window. </p>
          <p>That exploration may spur me to update the Source in some way, which I can do from right within the IDE,
            and immediately re-execute the query. This sort of read-eval-print loop enables fast iteration in a way that
            simply isn‚Äôt possible with LookML or any other semantic layer that requires both SQL and YAML for
            configuration (and even worse, oftentimes a separate API for actually querying the data). Malloy‚Äôs semantic
            layer marries two separate but intricately related disciplines ‚Äì exploring data and codifying rules around
            it. This not only removes the overhead of context switching between the two tasks, but it actually improves
            the individual experience of each. Implementing a data model actively improves the data exploration
            experience, and vice versa. </p>
          <p>While the semantic layer is the main attraction for me, there are many other noteworthy features. One
            particularly nice feature is its handling of nested data. Rollup queries with nested subtotals can be
            painful to write in SQL, and the
            <code><a
              href="https://docs.snowflake.com/en/sql-reference/constructs/group-by-rollup.html">GROUP BY ROLLUP</a></code> function in most SQL
            dialects produces awkward output that is very difficult to work with. Malloy‚Äôs nest clause makes this
            trivial. The previous query simply counts the number of flights by month. Let‚Äôs say I now want to compute
            the top 3 destination airports for each month: </p>
          <p>
            <a href="#img-malloy-nest">
              <img loading="lazy" alt="" id="malloy-nest" src="./img/malloy_nest.png" style="max-width: 100%" /></a>
          </p>
          <p>For each row in the original by-month query, there‚Äôs a new column that contains a nested table with flight
            counts by destination. Note also that since the original flights Source has pre-defined the join with
            destination, accessing fields from the destination table is as simple as referencing them with dot notation:
            destination.name in the above example. Getting this same information in a single SQL query is quite a bit
            more verbose and requires either a window function or a self-join: </p>
          <p><code>
<pre>WITH flights_by_destination_and_month AS (
  SELECT
    DATE_TRUNC(‚Äòmonth‚Äô, dep_time) AS dep_month,
    d.name AS destination_name,
    COUNT(*) AS flight_count
  FROM flights f
  LEFT JOIN destination d
  GROUP BY 1,2
)
SELECT
  dep_month,
  destination_name,
  SUM(flight_count) OVER (PARTITION BY dep_month) 
    AS flight_count_by_month,
  flight_count AS flight_count_by_dest_and_month,
FROM flights_by_destination_and_month
QUALIFY ROW_NUMBER() OVER (
  PARTITION BY dep_month ORDER BY flight_count DESC
) <= 3
ORDER BY 1,2
</pre></code>
          </p>
          <p> There‚Äôs a lot more to Malloy than what I‚Äôve shared here, so I encourage you to take a look at the <a
              href="https://github.com/looker-open-source/malloy">Github repository</a> and the <a
              href="https://looker-open-source.github.io/malloy/documentation/language/basic.html">documentation</a>.
          </p>
          <h3>Summary</h3>
          <p> Could Malloy finally be the language that replaces SQL? It‚Äôs a nigh impossible task, but I am very much
            hoping that it succeeds. Unlike other projects that only seek to improve on the aesthetics of the language,
            Malloy takes a more ambitious approach and tackles a critical missing piece of the stack. Its marriage of
            query language and semantic layer has the potential to radically change the disciplines of analytics and
            business intelligence for the better. Other trends, such as the rise and dominance of a few data platform
            products, namely BigQuery and Snowflake, mean that for Malloy to really succeed, there are relatively few
            database targets that it must support. It‚Äôs still very early days for the project, and who knows where it
            will go from here, but this is one that I‚Äôll be keeping a very close eye on. </p>
        </div>
      </article>
      <article>
        <label for="sql-critique" class="post-title">A Critique of SQL, 40 Years Later</label>
        <a href="?postid=sql-critique#blog" class="selflink">[link]</a>
        <time datetime="2022-08-11">08.11.2022</time>
        <input type="checkbox" id="sql-critique" />
        <div class="post-body">
          <p><i>Author's note: this post has found it's way to the front-page of Hacker News -- follow along with the <a
                href="https://news.ycombinator.com/item?id=32578725">discussion there</a>.</i></p>
          <p>The SQL language made its first appearance in 1974, as part of IBM‚Äôs System R database. It is now nearly 50
            years later, and SQL is the de facto language for operating the majority of industrial grade databases. Its
            usage has bifurcated into two domains ‚Äì application programming and data analysis. The majority of my 12
            year career (data engineer and data scientist) has been concerned with the latter, and SQL is by far the
            language that I have used the most. I love SQL for the productivity it has afforded me, but over time I‚Äôve
            also become aware of its many flaws and idiosyncrasies. My perspective is primarily from a practitioner‚Äôs
            standpoint, and I have always been curious if those ‚Äúreal world‚Äù issues have more fundamental or theoretical
            underpinnings. This brought me to <a
              href='https://courses.cs.duke.edu/compsci516/cps216/spring03/papers/date-1983.pdf'>A Critique of the SQL
              Database Language</a>, by mathematician and computer scientist CJ Date. Date was a former IBM employee, a
            well known database researcher, and friend of EF Codd. The SQL standard has received many major updates
            since this critique was first published, but which of those critiques are still valid today? </p>
          <p> A Critique of the SQL Database Language was first published in November 1984 in The ACM SIGMOD Record. It
            examines the dialect of SQL implemented by several IBM systems (SQL/DS, DB2, and QMF) which provided the
            basis for the initial SQL standard. Having no direct experience with any of these systems, reading the SQL
            examples from the paper is a bit like trying to read 17th century English ‚Äì it has a stilting, unfamiliar
            cadence that requires an extra bit of effort to understand. In the examples below, I‚Äôll use the terms
            SQL[1983] and SQL[2022] to distinguish between the older dialect, and what is available today. Use of the
            unqualified term ‚ÄúSQL‚Äù means my comment could apply to both. </p>
          <p> The paper consists of eight sections, each one describing a different category of criticism: lack of
            orthogonality in expressions, lack of orthogonality in functions, miscellaneous lack of orthogonality,
            formal definition, mismatch with host language, missing functions, mistakes, and missing aspects of the
            relational model. In the rest of this post, I‚Äôll go through each of those sections, describe the critique in
            informal terms, and give my interpretation on whether the critique is still relevant. </p>
          <h3>Lack of Orthogonality: Expressions</h3>
          <p> Orthogonality with respect to programming languages means roughly that the constructs of the language are
            like Lego blocks ‚Äì a small number of basic pieces can be recombined in simple and intuitive ways. Lack of
            orthogonality (again, informally speaking) means the language has lots of special cases and exceptions in
            how the components can be put together, which make it complex to learn and unintuitive to use. </p>
          <p> This section begins with a definition of table-expression, column-expression, row-expression, and
            scalar-expression. Respectively, these are expressions in SQL that return a table, column, row, and scalar
            value. In SQL[1983], the <code>FROM</code> clause of a <code>SELECT</code> statement was restricted to only
            specifying table or view names, and not general table-expressions, i.e., subqueries or common-table
            expressions (CTE). This made constructing nested expressions, one of the key features of Relational Algebra,
            nearly impossible. Modern SQL provides the capability to reference a CTE or subquery in a <code>FROM</code>
            clause, so this concern is mostly irrelevant today; however, the idea that a table-expression can take the
            form of ‚Äútablename‚Äù in some contexts, but must be <code>SELECT * FROM tablename</code> in others is
            interesting.</p>
          <p>For example, why not allow the following expression as a legal statement:</p>
          <p><code><pre>tablename;</pre></code></p>
          <p>which would return identical results to:</p>
          <p><code><pre>SELECT * FROM tablename;</pre></code></p>
          <p>Both are table-expressions (statements that return a table), and thus should be allowed anywhere that
            accepts a table-expression, e.g., the <code>FROM</code> clause of a <code>SELECT</code> statement, or a
            statement itself. </p>
          <p> While <code>SELECT</code> statements in SQL[1983] are not allowed in the <code>FROM</code> clause, they
            are required as an argument to an <code>EXISTS</code> clause. Furthermore, the <code>SELECT</code> statement
            here is required to be a column-expression (selecting only a single column) ‚Äì a statement that returns a
            table, a row, or a scalar will not work. When is a <code>SELECT</code> statement a table-expression, a
            column-expression, a row-expression or a scalar-expression? The language itself provides no guidance here,
            and it is wholly dependent on the query itself; e.g.: </p>
          <p><code><pre>SELECT a FROM tablename;</pre></code> </p>
          <p> is a column-expression, but </p>
          <p>
            <code><pre>SELECT a,b FROM tablename;</pre></code>
          </p>
          <p> is a table-expression. This bit of arbitrariness still exists in SQL[2022]. </p>
          <h3>Lack of Orthogonality: Functions</h3>
          <p>While some of the concerns in this section are mitigated by the introduction of subqueries and CTEs, a lot
            of them still hold true today. Column functions in SQL take a column of scalars as input, and return either
            a column of scalar values (e.g., the MD5 function or a type-casting function), or a single scalar (e.g.,
            aggregate functions like <code>SUM</code>). The author argues here that since column functions take a column
            of scalar values as input, any valid column-expression should be allowed. An example where this is not the
            case is as follows:</p>
          <p><code><pre>SELECT SUM(val) FROM tbl</pre></code></p>
          <p>is allowed, but </p>
          <p><code><pre>SELECT SUM( SELECT val FROM tbl )</pre></code></p>
          <p>is not, even though <code>SELECT val FROM tbl</code> is a valid column-expression ‚Äì it returns a single
            column, <code>val</code>, from table <code>tbl</code>.</p>
          <p>The key problem here is that the input to the <code>SUM</code> function in the first example is a column
            name, but that column name alone does not define the column-expression. Instead, we must look at the context
            (i.e., the full query) to understand that the ‚Äúval‚Äù column comes from ‚Äútable‚Äù. Said another way, in SQL,
            F(X) is not dependent only on X, but on contextual information surrounding F:</p>
          <p><code><pre>SELECT SUM(amount) FROM purchases;</pre></code></p>
          <p>and</p>
          <p><code><pre>SELECT SUM(amount) FROM purchases WHERE 1 = 0;</pre></code></p>
          <p>Are two very different queries, even though the column-function invocation <code>SUM(amount)</code> is
            identical.</p>
          <p>This also makes it difficult to nest aggregations. Consider the following example: we have a database of
            purchases for an ecommerce website, and want to retrieve (1) the total amount spent for each customer, and
            (2) the average spend across all customers. SQL[1983] could not solve this in a single statement. SQL[2022]
            can solve it with the use of CTEs:</p>
          <p><code><pre>
  WITH spend_per_customer AS (
    SELECT
      SUM(amount) AS customer_total
    FROM purchases
    GROUP BY customer
  )
  SELECT AVG(customer_total) FROM spend_per_customer</pre></code>
          </p>
          <p>However, the following (arguably more natural) statement is not allowed:</p>
          <p><code><pre>
  SELECT
    AVG(
        SELECT SUM(amount) FROM purchases GROUP BY customer
    )</pre></code>
          </p>
          <p>In the above query, the inner <code>SELECT</code> is a column-expression (<code>SELECT</code> statement
            returning a single column), and <code>AVG</code> is a function that takes a single column; however, the
            above statement does not work in most databases. In Snowflake, the above query responds with the error
            message ‚ÄúSingle-row subquery returns more than one row‚Äù, which I find confusing, since the <code>AVG</code>
            function clearly expects input of more than one row. </p>
          <p>Another interesting consequence here is the necessity of the <code>HAVING</code> clause. The
            <code>HAVING</code> clause is a favorite ‚Äúgotcha‚Äù of SQL interviewers everywhere. Why and how is it
            different from a <code>WHERE</code> clause? The answer is not immediately obvious to someone looking at SQL
            for the first time. Specialized knowledge like this certainly serves a purpose as an indicator for
            experience, but it can just as easily be seen as a deficiency of the SQL language. The <code>HAVING</code>
            clause provides a scoping hint to a column-function to indicate that the function input must make use of the
            <code>GROUP BY</code> clause. The author does not mince words here: ‚ÄúThe <code>HAVING</code> clause and the
            <code>GROUP BY</code> clause are needed in SQL only as a consequence of the column-function argument scoping
            rules.‚Äù </p>
          <p>The author also describes table-functions (functions that take a table as input, rather than just a
            column), and laments several instances of arbitrary and non-orthogonal syntax. First, the
            <code>EXISTS</code> function (takes a table-expression, returns a scalar) can only be used in a
            <code>WHERE</code> clause, whereas orthogonality would dictate that it should be allowed anywhere that the
            language accepts a scalar. Second, the <code>UNION</code> function is represented by an in-fix operator, and
            since SQL[1983] did not allow arbitrary table-expressions in <code>FROM</code> clauses, it was impossible to
            compute a column-function over a <code>UNION</code> of two tables. This problem is solved in SQL[2022], as
            the following syntax is now legal: </p>
          <p>
            <code>
  <pre>
  SELECT
    SUM(val)
  FROM (
    SELECT val FROM instore_purchases
    UNION ALL
    SELECT val FROM online_purchases
  )</pre>
  </code>
          </p>
          <h3>Lack of Orthogonality: Miscellaneous Items</h3>
          <p>This section contains a grab-bag of items related to functionality and implementation details of the
            underlying systems ‚Äì host/indicator variables, cursors, ‚Äúlong‚Äù fields (e.g., character fields with length
            greater than 254). Some of the limitations are indeed very frightening (a ‚Äúlong‚Äù field could not be
            referenced in a <code>WHERE</code> or <code>GROUP BY</code> clause!), but modern database systems are no
            longer subject to these restrictions. Other items in this section have been addressed by updates to the SQL
            standard. In no particular order, the following limitations are no longer applicable: </p>
          <ul>
            <li> Only simple expressions (column names) allowed in <code>GROUP BY</code></li>
            <li>NULL literal could not be used in places where a scalar constant was expected</li>
            <li>No concept of <code>UNION ALL</code></li>
            <li>Only possible to aggregate at one level with the <code>GROUP BY</code> construct</li>
          </ul>
          <p> While much of the discussion here is no longer relevant, the discussion of <code>NULL</code> values
            remains as scary today as it ever was. Inconsistency in <code>NULL</code> handling gives rise to some truly
            unexpected and frightening results, most notably in aggregate functions. Aggregate functions ignore
            <code>NULL</code> values, leading to the unfortunate fact that for a column X with values
            <code>x1, x2, ‚Ä¶, xn</code>, <code><pre>x1 + x2 + ‚Ä¶ + xn != SUM(X)</pre></code>and
            <code><pre>(X1 + X2) != SUM(X1) + SUM(X2)</pre></code>See the <a
              href="https://www.db-fiddle.com/f/hUeLXcYP38eEqZmvLbdumZ/1">following example</a> in Postgres: </p>
          <p><code><pre>
  WITH v AS (
    SELECT * FROM ( 
        VALUES 
          (1, 5),
          (null, 10) 
      ) AS t (column1, column2) 
  )
  SELECT 
    SUM(column1 + column2) AS sum_of_addition 
    , SUM(column1) + SUM(column2) AS addition_of_sum 
  FROM v;</pre>
          </code></p>
          <p>which outputs</p>
          <p><code><pre>
    sum_of_addition | addition_of_sum 
   -----------------+-----------------
                  6 |              16
   (1 row)
          </pre></code></p>
          <h3>Formal Definition, Mismatch with Host Language, and Missing Functions</h3>
          <p>These three sections are taken together, as I found none of them to be of particular relevance to modern
            databases, modern SQL, or analytical query processing.</p>
          <p><em><b>Formal Definition: </b></em>This section highlights areas where the developing SQL[1983] standard
            either disagreed with the IBM implementation, or was not precise enough ‚Äì cursor positioning, lock
            statements, alias scoping rules, and more. I understand this section more to be a critique of the standard,
            as opposed to the language itself. Furthermore, many of these issues (cursors, locks) are not as relevant to
            analytical processing, and are thus not as interesting to me personally.</p>
          <p><em><b>Mismatch with Host Language: </b></em>Similar to the previous section, I found this one mostly
            irrelevant. The author points out many differences between SQL and the host language (e.g., IBM PL/I) that
            cause friction for the programmer. Today, there are so many potential host languages (Python, Ruby,
            Javascript. Java just to name a few), each with their own idiosyncrasies, that it would be impossible for
            SQL to conform to all of them. Technologies like <a
              href="https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/">LINQ</a> aim to
            address some of these concerns, but as with above, these primarily target application programming use cases.
          </p>
          <p><em><b>Missing Functions: </b></em>Most of the bullet points here are related to cursors and locking, which
            I view as implementation-specific details related to underlying systems.</p>
          <h3>Mistakes</h3>
          <p>This section describes several things that the author views as simply a mistake in the language design.
            Here again, <code>NULL</code> is the prime example:</p>
          <blockquote>
            <p>In my opinion the null value concept is far more trouble than it is worth‚Ä¶ The system should never
              produce a (spuriously) precise answer to a query when the data involved in that query is itself imprecise.
              At least the system should offer the user the explicit option either to ignore nulls or to treat their
              presence as an exception</p>
          </blockquote>
          <p>It is interesting to note that this was far from the consensus view, even amongst the original developers
            of the relational model. EF Codd himself sanctioned the use of <code>NULL</code> in his <a
              href="https://en.wikipedia.org/wiki/Codd%27s_12_rules">12 Rules</a> (rule no. 3).</p>
          <p>Other "mistakes" included are:</p>
          <ul>
            <li>Primary Key is specified as part of an Index as opposed to at table creation time.<ul>
                <li>The reasoning here is that a Primary Key is really a logical property of a table, and should not be
                  intermingled with an index, which deals primarily with the physical access path of that data. Today,
                  most databases allow a <code>CREATE TABLE</code> statement to include a Primary Key, so this concern
                  has largely been rectified. </li>
              </ul>
            </li>
            <li> <code>SELECT *</code> is undoubtedly convenient for interactive querying, but extremely prone to errors
              when used in programs. <ul>
                <li>Date argues that <code>SELECT *</code> should only be allowed in interactive sessions. I largely
                  agree with this sentiment, but defining ‚Äúinteractive session‚Äù is by no means a trivial problem.</li>
              </ul>
            </li>
          </ul>
          <h3>Aspects of the Relational Model Not Supported</h3>
          <p>This section is another list of miscellaneous items, unified by the fact that each of them prevented
            SQL[1983] from truly being ‚Äúrelational‚Äù.</p>
          <p><em><b>Primary Keys and Foreign Keys:</b></em> Primary Keys could easily be ignored by SQL[1983] and
            Foreign Keys did not even exist. While SQL[2022] does allow for Foreign Keys, and many databases enforce
            referential integrity, SQL[2022] still does not fully understand the semantics of Primary Keys and Foreign
            Keys. Two examples:</p>
          <ul>
            <li>When performing a <code>GROUP BY</code> on the Primary Key of a table, and including other columns from
              that table, because the Primary Key guarantees uniqueness, it is guaranteed that those other columns will
              also be unique; however, SQL requires that those columns also be included in the <code>GROUP BY</code>.
            </li>
            <li>A join between a Foreign Key and its corresponding Primary Key could easily be implicit, but SQL still
              requires the join condition to be explicitly written out.</li>
          </ul>
          <p><em><b>Domains:</b> </em>Domain is another word for ‚Äútype‚Äù. Type systems in SQL[1983] only permitted
            primitive types (int, char, float, etc.). Today, Postgres provides support for user-defined types of
            arbitrary complexity, as well as check-constraints that allow users to restrict primitive types to
            acceptable values. Unfortunately, most OLAP data warehouses don‚Äôt support user-defined types, and SQL itself
            doesn‚Äôt have much to say on the topic.</p>
          <p>To take a simple example of how this can be dangerous, many databases in the wild have tables with integer
            Primary Key ID columns. Clearly not all of the operations that are legal for integers should be allowed on
            Primary Key columns ‚Äì what does it mean to add, multiply, or divide two PK IDs? SQL, and most databases,
            will happily let you perform these operations.</p>
          <p>
            <em><b>Relation Assignment: </b></em>The critique here is a single sentence ‚Äì
          </p>
          <blockquote>
            <p>A limited form of relation assignment is supported via <code>INSERT ... SELECT</code>, but that operation
              does not overwrite the previous content of the target table, and the source of the assignment cannot be an
              arbitrary algebraic expression (or <code>SELECT</code> equivalent).</p>
          </blockquote>
          <p>This is no longer true. Relation assignment can be done via <code>CREATE OR REPLACE TABLE AS</code>. With
            subqueries and CTEs, the source can be any arbitrary algebraic expression.</p>
          <p><em><b>Explicit <code>JOIN</code>, <code>INTERSECT</code>, and <code>DIFFERENCE</code> operators:</b></em>
            SQL[1983] did not support these. SQL[2022] does. JOIN was added to the SQL92 standard.
            <code>INTERSECT</code> and <code>MINUS</code> are supported by most databases, and even if they aren‚Äôt, the
            operators have semantically identical equivalents using <code>JOIN</code>. </p>
          <h3>Summary</h3>
          <p>While many of the critiques of SQL have been fixed by updates to the ANSI standard, many are still present.
            Lack of orthogonality in many places still exists, which makes SQL clunky to learn and use; however, I
            suspect the learning curve here is not actually all that high, judging by the number of people out there who
            can write SQL. By contrast, missing components of the relational model and issues arising from
            <code>NULL</code> values are likely the cause of many queries that look correct but provide wrong answers,
            especially by folks who are confident in their ability to write queries, but unfamiliar with some of the
            nastier traps. </p>
          <p>Despite the improvements listed above, in a <a
              href="https://www.red-gate.com/simple-talk/opinion/opinion-pieces/chris-date-and-the-relational-model/">2014
              interview</a>, CJ Date said ‚Äúwe didn‚Äôt realize how truly awful SQL was or would turn out to be (note that
            it‚Äôs much worse now than it was then, though it was pretty bad right from the outset).‚Äù This quote leaves me
            wondering ‚Äì if Date himself were to write an updated critique, what would it look like? My best guess is
            most of his criticism would revolve around further departures of SQL from the relational model, but specific
            examples escape me.</p>
          <p>SQL‚Äôs market dominance means every DBMS vendor is strongly incentivized to implement a SQL interface, and
            every aspiring programmer must learn it. So does this mean that despite all its problems, we‚Äôre stuck with
            SQL for good? I think SQL will continue to live on in some form for a very long time, probably even as the
            dominant query language; however, I strongly believe there‚Äôs still room for the development of new query
            languages that have learned the lessons of the past. Furthermore, I think the time is now better than ever
            for such a language to succeed. My reasons for believing so are beyond the scope of this essay, perhaps a
            good topic for the next one.</p>
        </div>
      </article>
      <article>
        <label for="post1" class="post-title">My Time on the Job Market as a Data Engineer</label>
        <a href="?postid=post1#blog" class="selflink">[link]</a>
        <time datetime="2019-04-17">04.17.2019</time>
        <input type="checkbox" id="post1" />
        <div class="post-body">
          <p> In December 2018, I had an amazing job at a company I loved, doing work I was immensely proud of. This
            made my decision to quit extraordinarily difficult. This decision could itself be the topic of a long and
            rambling blog post, but that's a story for another day. In this post, I'll talk through my experience being
            unemployed, my approach to the job search, and how I ultimately made the decision for my next career move.
          </p>
          <h3>Unemployment</h3>
          <p> I left my prior job without a new one lined up. When my friends heard I was entering a phase of
            funemployment, they all assumed I would be taking off to backpack around the world for 6 months. I take that
            to mean that I do a pretty good job of hiding my true workaholic anxious nature. By the time the New Year
            rolled around, I was already irrationally worrying about my employability and felt like I was behind
            schedule with everything ‚Äî interview prep, networking, and everything in-between. Soliciting advice from a
            few friends who had taken extended time off, I was able to assuage the rising feeling of panic and settle
            into a loose routine for my time off. I would get a full night's sleep every day, spend mornings catching up
            on the latest tech news and analysis (I read a LOT of Stratechery), and take at least one weekday each week
            to do absolutely nothing job related (this usually meant riding my bike all day). The rest of the time would
            be split between catching up with old friends and former colleagues, gathering information about companies I
            was interested in, and practicing my programming and technical whiteboarding skills. </p>
          <h3>The Job Search</h3>
          <p> From both my experience as a hiring manager as well as preliminary conversations with a few peers, I
            understood very quickly that it was a buyer's market, and I could easily make job hunting a full time
            effort. I also wanted to give myself the opportunity to truly evaluate a broad spectrum of companies of all
            shapes and sizes, across many different industries. In total, I spoke to 26 different companies, had tech
            screens with 11 (withdrew my application from the rest), did 9 onsite interviews, and ended up with 7
            offers, the majority of which were for individual contributor roles as a data engineer. </p>
          <p> I thought this process would be exhausting, but it turned out to be far more fun and exciting than I
            anticipated. It was fascinating to learn about all the different organizations and businesses, and the
            unique challenges faced by each of them. As I talked to more and more people, I started to develop a better
            sense of how to extract real signal from my conversations. During interviews, most folks are either in
            evaluation mode or sales mode. In both cases, they're likely to stick to an HR-approved script. My goal in
            every interview was to build enough rapport with the interviewer that I could successfully navigate the
            conversation away from the typical clich√©s. I also did my best to ask very similar questions to each
            interviewer. By listening closely to their individual answers, I could then evaluate them each in the
            context of the whole, which would often paint a much more telling picture of the organization than any
            individual answer. Do individual contributors understand the vision set forth by their managers? Are the
            pains of the ICs being heard by management? Are different business units aligned on the company mission?
          </p>
          <p> Every company I talked to had extremely aggressive hiring goals. Most were looking to double their
            engineering headcount by the end of the year, and more than double the size of their data engineering teams.
            More often than not, when I asked engineering leaders about their biggest challenges, hiring was #1 on the
            list. I began to evaluate prospective companies through this lens, asking ‚Äúhow will this company
            differentiate from all the others when competing for talent?‚Äù Every company had a different angle for this,
            some leveraging recent fundraising events or a high profile consumer brand, others leaning heavily on their
            social-impact oriented mission. I tried to understand not only how their answers appealed to me, but how
            they might appeal to the broader segment of job-seekers. </p>
          <h3> Key Takeaways </h3>
          <p> I learned a lot during my interviews. Rather than try and tie them all together into a neat narrative,
            I'll just list a few things that stood out to me as noteworthy: </p>
          <ul>
            <li> The technical bar for data engineering is reasonable. I did quite a bit of prep using the standard
              books and websites like Cracking the Coding Interview and leetcode. Never was I asked anything I felt was
              overly difficult or unfair. </li>
            <li> I don't consider myself a great programmer, but do think I have better than average soft skills for an
              engineer. Based on my success during the interview process, I suspect this combination is more valuable
              than the inverse. </li>
            <li> A 5 hour onsite interview is simply not enough time to effectively evaluate a workplace. During 5 hours
              of interviewing, a candidate has at most 1 hour available for asking questions about the company. How can
              someone possibly learn enough about a company during that time to make an informed decision? Doing
              pre-interview prep and intelligence gathering is absolutely critical, as is being efficient with your
              time. </li>
            <li> I really enjoyed all of my conversations with companies, except declining offers. It's emotionally
              draining to let someone down immediately after they've congratulated you and told you how excited everyone
              is about the possibility of you joining. It also forced me to confront the reality that the choice to go
              through one door meant closing many others. </li>
          </ul>
          <h3>The Decision</h3>
          <p> I count myself as extraordinarily fortunate to have had my pick of some of the best technology companies
            in San Francisco. I was looking for a company with aggressive growth, a great product, and awesome
            leadership, and while many of the companies I talked to met these criteria, Snowflake was a clear cut above.
            When I first started using Snowflake as a customer at my previous job, I was totally blown away by their
            product. The Snowflake data warehouse was critical to my job as a data engineer, and it was obvious to me
            how revolutionary a technology they had developed.</p>
          <p> The job at Snowflake was in sales engineering, a big change from my prior role as an in-house data
            engineer. As a sales engineer, the responsibilities are primarily around evaluating the data architecture of
            potential customers, helping prove out the value of Snowflake within that architecture, and scoping and
            executing on a proof-of-concept. The chance to get a glimpse of data teams of all shapes and sizes across
            the San Francisco tech scene and beyond seemed like a unique opportunity. </p>
          <p> From a team perspective, I knew Snowflake's sales and sales engineering org fairly well from my time as a
            customer. Both groups were great to work with ‚Äî their sales engineering lead was enormously valuable in
            helping us with our initial implementation, and the regional sales director struck me as an ambitious,
            driven individual who would likely push me to realize more of my potential. This gave me a high degree of
            confidence in the general quality of the team over at Snowflake, which was confirmed yet again during my
            interview process. </p>
          <p> As I alluded to earlier, I spent a fair amount of my funemployment reading through the back catalogue of
            Ben Thompson's Stratechery blog. Stratechery focuses primarily on consumer technology, with decidedly fewer
            articles on enterprise software, especially a product as technical as Snowflake. Even so, many of the themes
            he emphasizes over and over when discussing consumer tech apply just as well to enterprise. In this light,
            many of Snowflake's initiatives made sense as part of a broader strategy. I didn't see any other players in
            the space operating at the same level, and this combination of superior product and thought leadership made
            it an extremely compelling opportunity. </p>
          <p> I'm only a few weeks into my new role as a sales engineer at Snowflake, and so far it has not
            disappointed. The energy around what we're building, both in terms of the product and the business is
            absolutely incredible. Funemployment is finally over, but now the real fun begins! </p>
        </div>
      </article>
      <article>
        <label for="post2" class="post-title">Fort Bragg 600k Ride Report</label>
        <a href="?postid=post2#blog" class="selflink">[link]</a>
        <time datetime="2017-05-13">05.13.2017</time>
        <input type="checkbox" id="post2" />
        <div class="post-body">
          <p> The crown jewel of the <a href="https://www.sfrandonneurs.org/">San Francisco Randonneurs</a> brevet
            series is the Fort Bragg 600k. I‚Äôd never ridden a 600k before, and this would be my longest ride by a good
            margin. I was confident my legs were strong enough, but a 600k is ridden on the strength of a rider‚Äôs
            stomach, not his legs. The real challenge would be keeping myself adequately fueled while preventing my
            stomach from revolting.</p>
          <p> To that end, I enlisted the help of <a href="https://www.hammernutrition.com/perpetuem">Hammer
              Perpetuem</a>, a powdered energy drink mix. One scoop of Perpetuem, 70 grams, delivers 135 calories -- 87%
            simple carbohydrates in the form of maltodextrin (simple carbohydrate energy source), 10% soy protein (to
            prevent cannibalization of lean muscle tissue), and 3% fat. It‚Äôs a bland substance, without the ultra-sweet
            kick of energy gels, and a consistency somewhat reminiscent of sidewalk chalk, but it keeps the engine
            running and burns relatively clean. I brought along 10 scoops, and would keep a water bottle filled with a
            2-scoop mix at all times. Theoretically enough Perpetuem for 10 hours of saddle time. </p>
          <p> Pt. Reyes Station is the first stop along the ride, and most other riders made the standard rush for a
            pastry at Bovine Bakery. As they heaped praises on their scones and danishes, I sipped my Strawberry-Vanilla
            Perpetuem in silence. By Petaluma, I‚Äôd nearly finished my second bottle, but was starting to crave salty
            foods. Unwilling to stray too far from my liquid diet, I grabbed some string cheese to satisfy my craving,
            and diligently mixed up my third bottle of powdered fuel. As we rode to Healdsburg, the thought of fried
            chicken tenders from Safeway lodged itself stubbornly in my imagination. By the time I‚Äôd reached my fourth
            bottle of Perpetuem, each successive sip was becoming more and more laborious. When eating on the bike
            becomes a chore, it‚Äôs a sure sign that bad times are ahead. My fifth and final Perpetuem shake was meant to
            last all the way out to Fort Bragg at mile 182, but halfway through the bottle, the thought of drinking more
            induced mild nausea. Unable to eat take in calories on my preferred schedule, the 40 miles from Fort Bragg
            to the Indian Creek campground were an absolute slog. My legs began to run out of energy, but my stomach
            remained closed off, shutting out the possibility of refueling. </p>
          <p> I arrived at Indian Creek at around 10 PM in a ragged state, but was immediately welcomed by lots of
            familiar volunteer faces. I collapsed into a camp chair around a blazing fire, and was handed a variety of
            hot foods including homemade potato vegetable soup and crispy quesadillas. I had planned to sleep for around
            four hours at the campground, but really wanted to eat a full meal before going to bed to allow my body to
            digest and recover. I stared blankly into the distance, waiting for my appetite to recover. It never fully
            did, but I was able to force down the soup and one of the quesadillas before passing out in my tent. </p>
          <p> I closed my eyes for what felt like five seconds, but my 3:30 AM alarm rang loudly. Despite going to bed
            feeling pretty awful, I woke up in much better spirits. I wolfed down an egg and cheese sandwich, and was
            back on my bike, ready to go by 4:15. Riding through Anderson Valley just before dawn was undoubtedly the
            highlight of my ride. Rte 128 is beautiful, but plagued by an endless stream of cars shuttling back and
            forth from the coast. Combined with a small shoulder, it‚Äôs not a pleasant daytime ride. Before sunrise, I
            had the entire road all to myself, with a gibbous moon lighting the valley. </p>
          <p> Once I hit Healdsburg, I was essentially on autopilot. I deliberately chose to ride solo on the second day
            to relieve the pressure of trying to keep up with a group or pull through at an appropriate pace. With a
            more relaxed pace, I could also afford to be less draconian with my diet, and as an added bonus, I was
            completely out of Perpetuem. A few Clif bars and Larabars held me over until Freestone where I stopped in at
            <a href="https://wildflourbread.com/">Wild Flour bakery</a>. Coffee with two scones -- cheddar/bacon/onion
            and meyer lemon/cherry/almond -- did wonders for my mood. From Wild Flour, I‚Äôm a skip, hop and a jump away
            from San Francisco on familiar roads. I developed some pain in my right knee right after Pt Reyes Station
            (mile 330), but at that point, I felt so close to home that spirits were high, even limping along at a
            severely diminished pace. </p>
          <p> I rolled into the finish at Crissy Field just before 4 PM, greeted by a small crew of volunteers holding
            down the fort on a windy day at the shore. Too tired to socialize much, I grabbed a quick bite to eat,
            remounted my bike, and rode the final few miles back to my apartment. </p>
        </div>
      </article>
    </section>
    <section id="pbp">
      <!-- PORTFOLIO -->
      <div class="slides">
        <figure>
          <h1>Paris-Brest-Paris 2019 Ride Report</h1>
          <p> Paris-Brest-Paris has a credible claim to being the greatest cycling event in the world. If this is true,
            it's not because the route is the most beautiful or the most challenging (though it has its fair share of
            both), but because of the sheer number of people participating. Over 6,000 riders attempt the 1200 km
            course, and the drama, triumph, and despair of each individual ride is on full display to onlookers. </p>
          <p> No ride report can accurately recreate the experience of the ride, so rather than a play-by-play account,
            I'll instead tell a series of vignettes about my sensory experiences ‚Äî the sights, sounds, smells, tastes,
            and aches of my 88 hours on the bike. It's a terribly incomplete picture, but telling the full story is an
            impossible task. Paris-Brest-Paris is something you have to ride to truly understand. Scroll down to
            continue reading ‚¨áÔ∏è </p>
          <p style="margin-top: 1em">üö¥‚Äç‚ôÇÔ∏èüö¥‚Äç‚ôÄÔ∏èüö¥üèΩüö¥üèº‚Äç‚ôÇÔ∏èüö¥</p>
        </figure>
        <figure style="background: #00233f; color: #fdcb39">
          <h1>Lights</h1>
          <p> Riding PBP means riding at all times of day, including the dead of night. My wave rolled at 7:30pm, with
            the sun hanging inches above the horizon. The chaos and the rush of the start made it tough to appreciate
            the sunset, but night fell around us and my jitters calmed. We began to catch riders in prior start waves,
            and the infinite trail of red tail lights ahead looked like <span style="color: red">glowing coals</span>
            marking the way to Brest. Every now and then I'd be tempted to look back, but it was always a mistake.
            Modern headlights are <em>viciously bright</em>. Even a brief glance would make my eyes wince in pain and
            leave spots in my vision. I had to glean whatever information about the situation behind me by watching the
            shadows dance. When the lines of my shadow sharpened, it meant someone was approaching from behind. As they
            <span style="filter: blur(0.66px)">faded</span>, it meant we were separating. </p>
          <p> Three hours into the ride, I stopped on the side of the road to relieve myself. As I stretched my back and
            neck, I looked up at the sky and for the first time noticed the half-moon and the stars of the Milky Way
            above. I took a few extra seconds to soak in the beauty of the moment ‚Äî red lights marching on ahead, white
            lights slowly approaching from behind, and the cosmos above, twinkling softly like any other night. I
            remounted my bike and rejoined the endless stream of red and white making its way west. </p>
          <p style="margin-top: 1em">üö¥‚Äç‚ôÇÔ∏èüö¥‚Äç‚ôÄÔ∏èüö¥üèΩüö¥üèº‚Äç‚ôÇÔ∏èüö¥</p>
        </figure>
        <figure style="background: #001b02; color: #B5881B">
          <h1>Smells</h1>
          <p> At our first stop in the town of Mortagne-au-Perche, my first remark was ‚Äúthis whole town smells like
            brie.‚Äù Our evening start meant that for the first part of the ride, the French countryside was completely
            cloaked in darkness. Without visual stimulation aside from the lights of the other riders, my primary
            sensory experience was that of smell. </p>
          <p> Rain from the previous day had moistened the ground, and every forest, farm, and town that we sped through
            was an explosion of odors. Four primary smells left a major impression on me, and all but one I think was
            best described by a different type of cheese: brie, feta, and ch√®vre. The fourth was manure with some sour
            notes. Though none were unpleasant by any means, 770 miles of riding incurred fatigue in every part of my
            body, nose included. The occasional whiff of wild lavender was always a welcome reprieve, and early morning
            towns with boulangeries baking fresh croissants and baguettes were absolutely divine. </p>
          <p> Many US brevets travel on roads with moderate to heavy auto traffic, so I'm mostly accustomed to the
            smells of gasoline and diesel exhaust while I ride. I will take the smells of livestock, wildflowers, and
            pastries any day. </p>
          <p style="margin-top: 1em">üö¥‚Äç‚ôÇÔ∏èüö¥‚Äç‚ôÄÔ∏èüö¥üèΩüö¥üèº‚Äç‚ôÇÔ∏èüö¥</p>
        </figure>
        <figure style="background-color: #87CEEB; color: #293F47">
          <h1>Scenery</h1>
          <p> The PBP course rolls up and down through the tiny villages of Normandy and Brittany. The scenery, while at
            times quite lovely, can also become tiresome. The following pattern repeats endlessly: ride through a flat
            stretch lined with either cow pastures or corn fields. Turn into a small climb through a deciduous forest,
            and descend until you reach the outskirts of town. The road pitches up again, and cobblestone houses begin
            to appear, getting denser and denser until you reach the top of the hill, invariably marked by a large
            church. Begin the descent through the center of town over cobbled roads, past the boulangerie and the
            pharmacy, and before you know it, you're back in the corn fields. Qu√©dillac, M√©dr√©ac, Merl√©ac, R√©t√©ac,
            Loud√©ac. Rinse and repeat. </p>
          <p>
            <img loading="lazy" alt="Sunflowers on the return leg, near Louvigny" src="./img/pbp_scenery.jpg"
              style="max-width: 50%" />
          <figcaption style="margin-top: 0px">Sunflowers on the return leg, near Louvigny</figcaption>
          üö¥‚Äç‚ôÇÔ∏èüö¥‚Äç‚ôÄÔ∏èüö¥üèΩüö¥üèº‚Äç‚ôÇÔ∏èüö¥</p>
        </figure>
        <figure style="background-color: #FBC8D4; color: #293F47">
          <h1>Sounds</h1>
          <p>What does it sound like when 6000+ riders from every country on earth attempt to ride 1200 km?
          <ul style="font-size: 0.8em">
            <li> The steady whirring of my bike's drivetrain </li>
            <li> The creaks, squeaks, and rattles of the bikes owned by 6000 people who all have wildly varying bike
              maintenance standards </li>
            <li> The low rumble of an approaching peloton moving twice my speed. I nervously anticipate the rush of wind
              as they blow past </li>
            <li> Heavy labored breathing as we grind up the endless rolling hills on our way to Brest </li>
            <li> The sound of Ian's voice up ahead as he chatters happily away with one of our many friends on the ride
            </li>
            <li> Cheers of ‚ÄúBonne route!‚Äù and ‚ÄúBon courage!‚Äù from spectating French villagers </li>
            <li> Crinkling of space blankets as riders toss and turn, trying to steal a few precious minutes of sleep on
              the floor of a control point </li>
            <li> Constant chirping from Garmins, Wahoos, or other bike computers, alerting their owners to who knows
              what </li>
            <li> The clopping of bike cleats on asphalt and tile floors </li>
            <li> A cacophony of every language on Earth. French, German, English, Italian, Hindi, Korean, Japanese,
              Portuguese, Spanish, Chinese and more </li>
          </ul>
          </p>
          <p style="margin-top: 1em">üö¥‚Äç‚ôÇÔ∏èüö¥‚Äç‚ôÄÔ∏èüö¥üèΩüö¥üèº‚Äç‚ôÇÔ∏èüö¥</p>
        </figure>
        <figure id="discomforts" style="background-color: #ffa791; color: #293F47">
          <h1>Discomforts</h1>
          <p><span class=" previous">üõë</span> <span class="next">‚è≠Ô∏è</span></p>
          <p class="active entry">
            <b>Sunday 7:30 PM:</b> Feeling great. Start wave goes. Body feels amazing.
          </p>
          <p class="hidden entry">
            <b>Sunday 10 PM:</b> riding in same position for awhile in a fast group. Feel the typical aches associated
            with riding bikes. Some soreness in the saddle and a bit of numbness in toes, but nothing unusual. Knees
            holding up great. Finally catch up to Irving, Ian and Ben who started 30 minutes before me. Grateful for the
            chance to sit up and ride with them and stave off any potential muscle soreness from hammering with this
            group.
          </p>
          <p class="hidden entry">
            <b>Monday 4 AM:</b> rolling out of the control point at Villaine-ah-Juhel. We made the 200k mark in under 8
            hours and in celebration I over-ate. Bolognese topped with gruyere. Stomach not happy. Burps taste like
            gruyere.
          </p>
          <p class="hidden entry">
            <b>Monday 11 PM:</b> a few hours of sleep at the hotel in Loud√©ac. Getting up and I am stiff in the legs,
            but a few minutes on the bike and I'm feeling good again. 450 km in, knee is holding up.
          </p>
          <p class="hidden entry">
            <b>Tuesday 6 AM:</b> descending in freezing fog down into the town of Sizun. Cold air rushing over exposed
            skin is painful, but my time swimming in the SF bay with the Dolphin Club has made me far more tolerant of
            cold. Espresso and croissant in Sizun, and it's off to Brest.
          </p>
          <p class="hidden entry">
            <b>Tuesday 10 AM:</b> we reach Brest and I am ecstatic. 600 km in, and no physical problems, just fatigue
            and hunger.
          </p>
          <p class="hidden entry">
            <b>Tuesday 1 PM:</b> left knee starts acting up. Haven't had issues with this one before, so am getting
            worried. I stop, stretch out my legs, and attempt to manage the pain by easing back on the pace and trying
            different pedaling positions. Eventually start riding much slower, and with more emphasis on the right leg.
          </p>
          <p class="hidden entry">
            <b>Tuesday 6 PM:</b> left knee pain persists, but isn't getting too much worse from stop to stop. As
            expected, pedaling asymmetrically also starts to cause problems for my right knee. IT band begins to tighten
            up and generate soreness on my right side. Start limping at controls and having trouble walking down stairs.
          </p>
          <p class="hidden entry">
            <b>Tuesday 9 PM:</b> Right knee decently swollen and left knee still feeling moderate pain. Riding slowly
            has taken a huge toll on me. Without the rush of physical exertion, staying focused on the road takes an
            enormous amount of mental energy, leading to boredom and frustration. I swear up and down that I am never
            doing this, ever again. To stay awake and alert I fall back on caffeinated gels, which make me nervous and
            jittery. In a bout of frustration, just as we are hitting Loud√©ac for the second time, I turn up the gas.
            For a solid 15 minutes, I'm flying. Lots of pent up energy from the gels and caffeine. Passing dozens of
            riders like they are standing still, and momentarily the joy of riding returns. I stop briefly at a town
            corner and notice that even though I am not feeling tired or breathing heavily, my heart is absolutely
            racing. I stop, take thirty deep breaths, and try to down-regulate both my heart rate and my exhilaration.
            Oddly enough, both knees felt fine. Take a mental note of that, but fully expect this foolishness to put my
            knees further into a hole.
          </p>
          <p class="hidden entry">
            <b>Wednesday 2 AM:</b> wake up in the hotel, and everything is stiff as hell. Open my mouth to yawn and
            immediately yelp in pain due to chapped lips. Tongue and mouth are raw and abraded from constantly chewing
            crusty baguettes. Legs feel moderately better, but can tell that knees still won't be happy back on the
            bike. 450 km to go. How will I manage?
          </p>
          <p class="hidden entry">
            <b>Wednesday 10 AM:</b> the heat of the day is driving me crazy, and still bothersome knee pain means I'm
            moving super slowly. After 3 hours of crawling, I look back and a huge group of cyclists is coming up on us
            fast. I can't resist the temptation, and jump on the train and start hammering to keep up with them.
            Japanese power couple pulls us all the way to Villaine. Wait a second, knee suddenly feels great...
          </p>
          <p class="hidden entry">
            <b>Wednesday 4 PM:</b> Decide my knee only feels good when I'm hammering. The last two days of riding slowly
            means my legs have a good deal of snap left. Passing a continuous stream of riders, and begin collecting a
            decent sized train of riders in my slipstream. Get swept up by a group of twelve or so serious 84-hour
            riders and tuck in. This is some of the most fun I've ever had on a bike.
          </p>
          <p class="hidden entry">
            <b>Wednesday 11 PM:</b> arrive in Mortagne-au-Perche. Legs start to ache from the effort, but knees feel
            decent, especially after completing a regular regimen of hip and hamstring stretches. My renewed focus on
            riding hard means I'm paying less attention to my hands, and at this control I start to notice some of my
            fingers going numb, a classic symptom of ulner nerve compression. I'll take that any day over knee pain.
          </p>
          <p class="hidden entry">
            <b>Wednesday 3 AM:</b> leaving the Mortagne Airbnb, I start developing a pulsing headache, which I suspect
            is from not drinking enough water before going to sleep. Both knees still ache slightly, but somehow in a
            way that doesn't concern me.
          </p>
          <p class="hidden entry">
            <b>Wednesday 11 AM:</b> the finish. Nothing is terribly painful, but nothing feels great either. Contact
            points (butt, hands, feet), joints (knees, ankles), supporting tissues and small muscles (neck, Achilles)
            are all straining from nearly 90 hours of continuous effort. We are elated to have finished successfully,
            and all ignore the aches and pains in the glow of the finish.
          </p>
          <p class="hidden entry">
            <b>Thursday 8 AM:</b> the aftermath. The worst after-effects are chapped lips and sores/abrasions in my
            mouth and tongue, which make it hard to eat. Of course legs and butt are sore. Not about to jump on the bike
            again today, but after 1200 km of riding, I'm pleasantly surprised at how good I feel. I can sense the
            randonnesia setting in, and thoughts begin to form in my head ‚Äî what might a 2023 ride look like?
          <p style="margin-top: 1em">üö¥‚Äç‚ôÇÔ∏èüö¥‚Äç‚ôÄÔ∏èüö¥üèΩüö¥üèº‚Äç‚ôÇÔ∏èüö¥</p>
          </p>
        </figure>
      </div>
      <script>

        window.addEventListener("load", () => {

          // Allow for links directly to specific blog posts
          // e.g., https://carlineng.com/?postid=post1#blog
          const urlQueryString = window.location.search;

          if (urlQueryString) {
            const urlParams = new URLSearchParams(urlQueryString);
            const postId = urlParams.get('postid');
            if (postId) {
              const linkedBlog = document.querySelector(`#blog input#${postId}`);
              if (linkedBlog) {
                linkedBlog.checked = true;
              }
            }

            // Help the Lightbox return to a particular point in a post
            // Provide a `scrollto` parameter in the Lightbox return link --
            const scrollto = urlParams.get('scrollto');
            if (scrollto) {
              const elem = document.querySelector(`#${scrollto}`);
              if (elem) {
                elem.scrollIntoView({ block: "center", inline: "nearest" });
                const newUrl = window.location.pathname
                  + (postId ? `?postid=${postId}` : '')
                  + window.location.hash;
                window.history.replaceState(null, null, newUrl)
              }
            } else if (postId) {
              const parent = document.querySelector(`#blog input#${postId}`).parentNode;
              parent.scrollIntoView();
            }
          }
        });

        // Social Icon mover
        const socials = document.querySelector('div#social')
        socials.addEventListener('mouseover', (e) => {
          if (e.target !== e.currentTarget) {
            return;
          }

          // Snap element back if it gets close to bottom
          const topEdge = e.target.previousElementSibling.getBoundingClientRect().bottom;
          const { bottom: bottom, left: left, top: top } = e.target.getBoundingClientRect();
          if (bottom >= window.innerHeight * 0.98 || top <= (topEdge - 1)) {
            socials.style.transform = '';
            return;
          }

          const { offsetWidth: width, offsetHeight: height } = e.target;
          const { offsetX: x, offsetY: y } = e;

          const enterRight = 1 - ((width - x) / width);
          const enterLeft = (width - x) / width;
          const enterTop = 1 - ((height - y) / height);
          const enterBottom = (height - y) / height;

          let [pushX, pushY] = [0, 0];

          if (enterRight < enterLeft && enterRight < enterTop && enterRight < enterBottom) {
            pushX = 80;
          } else if (enterLeft < enterRight && enterLeft < enterTop && enterLeft < enterBottom) {
            pushX = -80;
          } else if (enterTop < enterLeft && enterTop < enterRight && enterTop < enterBottom) {
            pushY = 40;
          } else if (enterBottom < enterLeft && enterBottom < enterRight && enterBottom < enterTop) {
            pushY = -40;
          }

          socials.style.transform = `translate(${e.offsetX + pushX}px, ${e.offsetY + pushY}px)`;
        });

        socials.addEventListener('mouseleave', (e) => {
          socials.style.transform = '';
        });

        // Emoji carousel

        function randn() {
          var u = 0, v = 0;
          while (u === 0) u = Math.random(); //Converting [0,1) to (0,1)
          while (v === 0) v = Math.random();
          return Math.sqrt(-2.0 * Math.log(u)) * Math.cos(2.0 * Math.PI * v);
        }

        const carousel = document.querySelector('#emoji-carousel');

        const emojis = ['‚òï', 'üö¥‚Äç‚ôÇÔ∏è', 'üçú', 'ü•ü', 'üòµ‚Äçüí´', '‚ù§Ô∏è'];

        function switchEmoji() {
          if (randn() > 0.92) {
            const emoji = emojis[Math.round(Math.random() * 100) % (emojis.length)];
            carousel.textContent = emoji;
          }
        }

        setInterval(switchEmoji, 1000);

        // Controls for PBP Discomfort section
        const backButton = document.querySelector('#discomforts .previous');
        const nextButton = document.querySelector('#discomforts .next');

        function previousEntry() {
          const cur = document.querySelector('#discomforts .active.entry');
          const prev = cur.previousElementSibling;
          if (prev.classList.contains('entry')) {
            cur.classList.remove('active');
            cur.classList.add('hidden');

            prev.classList.remove('hidden');
            prev.classList.add('active');

            nextButton.textContent = '‚è≠';
            if (prev.previousElementSibling && prev.previousElementSibling.classList.contains('entry')) {
              backButton.textContent = '‚èÆ';
            } else {
              backButton.textContent = 'üõë';
            }
          }
        }

        function nextEntry() {
          const cur = document.querySelector("#discomforts .active.entry");
          const next = cur.nextElementSibling;
          if (next && next.classList.contains('entry')) {
            cur.classList.remove('active');
            cur.classList.add('hidden');

            next.classList.remove('hidden');
            next.classList.add('active');

            backButton.textContent = '‚èÆ';
            if (next.nextElementSibling && next.nextElementSibling.classList.contains('entry')) {
              nextButton.textContent = '‚è≠';
            } else {
              nextButton.textContent = 'üõë';
            }
          }
        }

        backButton.addEventListener('click', previousEntry);
        nextButton.addEventListener('click', nextEntry);

        window.addEventListener('keyup', e => {
          const curTarget = document.querySelector(':target');
          if (curTarget && curTarget.id === "pbp") {
            if (e.key === 'ArrowRight') {
              nextEntry();
            } else if (e.key === 'ArrowLeft') {
              previousEntry();
            }
          }
        });


      </script>
    </section>
    <section id="another-page">
      <!-- ANOTHER PAGE -->
      <p>This page is not referenced in the menu, yet it exists.</p>
      <p><a href="#about">‚Üê back</a></p>
    </section>
  </main>
  <!-- ----------
    LIGHTBOX IMAGES
    ----------- -->
  <!-- Other images -->
  <a href="#home" class="lightbox" id="img-home"><img loading="lazy" alt="" src="./img/site-image.webp" /></a>
  <a href="?postid=malloy-intro&scrollto=malloy-src#blog" class="lightbox" id="img-malloy-src"><img loading="lazy"
      alt="" src="./img/malloy_src.png" /></a>
  <a href="?postid=malloy-intro&scrollto=malloy-nest#blog" class="lightbox" id="img-malloy-nest"><img loading="lazy"
      alt="" src="./img/malloy_nest.png" /></a>
  <a href="?postid=sql-bad-syntax&scrollto=malloy-nest-result#blog" class="lightbox" id="img-malloy-nest-result"><img
      loading="lazy" alt="" src="./img/malloy_nest_result.png" /></a>
  <a href="?postid=sql-renaissance&scrollto=malloy-nest-result-2#blog" class="lightbox"
    id="img-malloy-nest-result-2"><img loading="lazy" alt="" src="./img/malloy_nest_result.png" /></a>
  <a href="?postid=sql-renaissance&scrollto=renaissance-cimabue#blog" class="lightbox" id="img-cimabue"><img
      loading="lazy" alt="" src="./img/cimabue-mary.jpeg" /></a>
  <a href="?postid=sql-renaissance&scrollto=renaissance-uccello#blog" class="lightbox" id="img-uccello"><img
      loading="lazy" alt="" src="./img/uccello-battle-of-san-romano.jpeg" /></a>
  <a href="?postid=sql-renaissance&scrollto=renaissance-davinci#blog" class="lightbox" id="img-davinci"><img
      loading="lazy" alt="" src="./img/davinci-annunciation.jpeg" /></a>
  <a href="?postid=malloy-tpcds&scrollto=tpcds-schema#blog" class="lightbox" id="img-tpcds-schema"><img loading="lazy"
      alt="" src="./img/tpcds_schema.png" /></a>
</body>

</html>